{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Essay Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim \n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can set up the dataframes and explore the data. We will drop columns that we don't need and those with NaN values. There was one row without a domain1_score, which I removed. Some essays also contained domain2 or domain3 scores, but since not all the data has that field, I will ignore that for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0              8  \n",
       "1              9  \n",
       "2              7  \n",
       "3             10  \n",
       "4              8  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.ExcelFile('./data/training_set_rel3.xls')\n",
    "df = data.parse(\"training_set\")\n",
    "df = df.drop('rater1_domain1', 1)\n",
    "df = df.drop('rater2_domain1', 1)\n",
    "df = df.dropna(axis = 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble! Thing about! Dont you think so? How would you feel if your teenager is always on the phone with friends! Do you ever time to chat with your friends or buisness partner about things. Well now - there's a new way to chat the computer, theirs plenty of sites on the internet to do so: @ORGANIZATION1, @ORGANIZATION2, @CAPS1, facebook, myspace ect. Just think now while your setting up meeting with your boss on the computer, your teenager is having fun on the phone not rushing to get off cause you want to use it. How did you learn about other countrys/states outside of yours? Well I have by computer/internet, it's a new way to learn about what going on in our time! You might think your child spends a lot of time on the computer, but ask them so question about the economy, sea floor spreading or even about the @DATE1's you'll be surprise at how much he/she knows. Believe it or not the computer is much interesting then in class all day reading out of books. If your child is home on your computer or at a local library, it's better than being out with friends being fresh, or being perpressured to doing something they know isnt right. You might not know where your child is, @CAPS2 forbidde in a hospital bed because of a drive-by. Rather than your child on the computer learning, chatting or just playing games, safe and sound in your home or community place. Now I hope you have reached a point to understand and agree with me, because computers can have great effects on you or child because it gives us time to chat with friends/new people, helps us learn about the globe and believe or not keeps us out of troble. Thank you for listening.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays = df['essay']\n",
    "essays[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that personally identifiying information has been replaces with @NER where NER is a NER tag. We can remove these symbols to avoid interferring with the spell checking counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.5 ms, sys: 25.2 ms, total: 76.7 ms\n",
      "Wall time: 76.1 ms\n"
     ]
    }
   ],
   "source": [
    "# Function to get all text from each essay - to build doc2vec\n",
    "def all_essays(df):\n",
    "    for (i, essay) in enumerate(df['essay']):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(essay), [i])\n",
    "        \n",
    "\n",
    "all_essay_lst = all_essays(df)\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(all_essay_lst)\n",
    "%time model.train(all_essay_lst, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features #TODO write description of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup pre-trained word2vec model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:57: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:60: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:64: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:67: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:70: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:73: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:76: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:80: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:81: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:82: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:83: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.shape[0]\n",
    "essays = df['essay'].values\n",
    "\n",
    "#initialize dataframe columns\n",
    "df['word_count'] = np.nan \n",
    "df['sentence_count'] = np.nan\n",
    "df['avg_word_length'] = np.nan \n",
    "df['num_exclamation_marks'] = np.nan\n",
    "df['num_question_marks'] = np.nan\n",
    "df['num_stopwords'] = np.nan\n",
    "df['word2vec_concat'] = np.nan\n",
    "\n",
    "df['verb_count'] = np.nan\n",
    "df['foreign_count'] = np.nan\n",
    "df['adj_count'] = np.nan\n",
    "df['conj_count'] = np.nan\n",
    "\n",
    "\n",
    "# TODO\n",
    "#df['num_advanced_words'] = np.nan\n",
    "#df['spelling_errors'] = np.nan\n",
    "\n",
    "def replace_punc(text):\n",
    "    return text.replace(\"@\", \"\").replace(\"%\", \"\")\n",
    "\n",
    "def get_pos_tags(essay):\n",
    "    nouns = verbs = foreign = adj = adv = conj = 0\n",
    "    tokens = nltk.word_tokenize(essay)\n",
    "    for token in tokens:\n",
    "        pos_tag = nltk.pos_tag(nltk.word_tokenize(token))\n",
    "        for (_, tag) in (pos_tag):\n",
    "            if tag[0] == \"N\":\n",
    "                nouns += 1\n",
    "            elif tag[0] == \"V\":\n",
    "                verbs += 1\n",
    "            elif tag[0:2] == \"FW\":\n",
    "                foreign += 1\n",
    "            elif tag[0] == \"J\":\n",
    "                adj += 1\n",
    "            elif tag[0] == \"R\":\n",
    "                adv += 1\n",
    "            elif tag[0:2] == \"CC\" or tag[0:2] == \"IN\":\n",
    "                conj += 1\n",
    "    \n",
    "    return [nouns, verbs, foreign, adj, adv, conj]\n",
    "\n",
    "\n",
    "for i in range(num_rows):\n",
    "    \n",
    "    # Remove placeholders\n",
    "    text = replace_punc(essays[i])\n",
    "    \n",
    "    # Turn essay into list of words\n",
    "    text = essays[i].split(\" \")\n",
    "    \n",
    "    # Set word count\n",
    "    df.set_value(i,'word_count', len(text))\n",
    "    \n",
    "    # Sentence count\n",
    "    df.set_value(i, 'sentence_count', len(nltk.tokenize.sent_tokenize(essays[i])))\n",
    "    \n",
    "    # Average word length\n",
    "    word_len = sum(len(word) for word in text) / len(text)\n",
    "    df.set_value(i, 'avg_word_length', word_len)\n",
    "    \n",
    "    # Number of exclamation marks\n",
    "    df.set_value(i, \"num_exclamation_marks\", sum(word.count(\"!\") for word in essays[i]))\n",
    "    \n",
    "    # Number of question marks\n",
    "    df.set_value(i, \"num_question_marks\", sum(word.count(\"?\") for word in essays[i]))\n",
    "    \n",
    "    # Number of stop words\n",
    "    df.set_value(i, \"num_stopwords\", sum([1 for word in text if word.lower() in stopwords]))\n",
    "\n",
    "    # Word2Vec conversion - min + max\n",
    "    df.set_value(i, 'word2vec_concat', min(model.docvecs[i]) + max(model.docvecs[i]))\n",
    "    \n",
    "    # POS tag counts\n",
    "    pos_lst = get_pos_tags(essays[i])\n",
    "    df.set_value(i,'verb_count', pos_lst[1])\n",
    "    df.set_value(i,'foreign_count', pos_lst[2])\n",
    "    df.set_value(i,'adj_count', pos_lst[3])\n",
    "    df.set_value(i,'conj_count', pos_lst[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>...</th>\n",
       "      <th>state</th>\n",
       "      <th>story</th>\n",
       "      <th>thing</th>\n",
       "      <th>things</th>\n",
       "      <th>think</th>\n",
       "      <th>time</th>\n",
       "      <th>use</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "      <td>338.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.550296</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234392</td>\n",
       "      <td>0.202706</td>\n",
       "      <td>0.193065</td>\n",
       "      <td>0.180893</td>\n",
       "      <td>0.247159</td>\n",
       "      <td>0.225527</td>\n",
       "      <td>0.196461</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "      <td>419.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.463007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "      <td>279.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217546</td>\n",
       "      <td>0.203830</td>\n",
       "      <td>0.278498</td>\n",
       "      <td>0.254123</td>\n",
       "      <td>0.221372</td>\n",
       "      <td>0.289028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "      <td>524.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.041985</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "      <td>465.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240730</td>\n",
       "      <td>0.209705</td>\n",
       "      <td>0.273795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  word_count  sentence_count  avg_word_length  \\\n",
       "0              8       338.0            16.0         4.550296   \n",
       "1              9       419.0            20.0         4.463007   \n",
       "2              7       279.0            14.0         4.526882   \n",
       "3             10       524.0            27.0         5.041985   \n",
       "4              8       465.0            30.0         4.526882   \n",
       "\n",
       "   num_exclamation_marks  num_question_marks  num_stopwords    ...     state  \\\n",
       "0                    4.0                 2.0          168.0    ...       0.0   \n",
       "1                    1.0                 1.0          189.0    ...       0.0   \n",
       "2                    0.0                 0.0          140.0    ...       0.0   \n",
       "3                    2.0                 1.0          222.0    ...       0.0   \n",
       "4                    0.0                 0.0          236.0    ...       0.0   \n",
       "\n",
       "   story     thing    things     think      time       use      want  \\\n",
       "0    0.0  0.234392  0.202706  0.193065  0.180893  0.247159  0.225527   \n",
       "1    0.0  0.000000  0.000000  0.000000  0.000000  0.259524  0.000000   \n",
       "2    0.0  0.000000  0.000000  0.217546  0.203830  0.278498  0.254123   \n",
       "3    0.0  0.000000  0.000000  0.000000  0.000000  0.258546  0.000000   \n",
       "4    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.240730   \n",
       "\n",
       "        way     world  \n",
       "0  0.196461  0.000000  \n",
       "1  0.000000  0.269336  \n",
       "2  0.221372  0.289028  \n",
       "3  0.000000  0.268321  \n",
       "4  0.209705  0.273795  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tfidf_vectors(essays):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_df=0.9, min_df=5, max_features=50, stop_words=\"english\", binary=True)\n",
    "    tfidf_vectors = vectorizer.fit_transform(essays)\n",
    "    new_df = pd.DataFrame(tfidf_vectors.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    return pd.concat([df, new_df], axis=1)\n",
    "\n",
    "df = get_tfidf_vectors(essays)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text data\n",
    "\n",
    "We need to convert the essay strings into some numerical form. We could use Word2Vec, a TF-IDF Vectorizer, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have added all the features, so now we can start to explore correlations between features and scores (to ensure we are making correct assumptions and to discover potential new features), and perform the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''         \n",
    "Worsened: 'word2vec_avg', 'noun_count','adj_count', 'adv_count', 'foreign_count',\n",
    "'''\n",
    "\n",
    "#features = df.drop('domain1_score', axis=1)\n",
    "x = df.drop(['domain1_score', 'essay'], axis=1)\n",
    "y = df['domain1_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
    "logistic_reg = LogisticRegression()\n",
    "logistic_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression classifier accuracy: 0.46\n"
     ]
    }
   ],
   "source": [
    "predictions = logistic_reg.predict(X_test)\n",
    "print('Logistic regression classifier accuracy: {:.2f}'.format(logistic_reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7985546474708368\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(predictions, y_test, weights=\"quadratic\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
