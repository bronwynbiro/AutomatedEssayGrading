{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Essay Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim \n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can set up the dataframes and explore the data. We will drop columns that we don't need and those with NaN values. There was one row without a domain1_score, which I removed. Some essays also contained domain2 or domain3 scores, but since not all the data has that field, I will ignore that for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0              8  \n",
       "1              9  \n",
       "2              7  \n",
       "3             10  \n",
       "4              8  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.ExcelFile('./data/training_set_rel3.xls')\n",
    "df = data.parse(\"training_set\")\n",
    "df = df.drop('rater1_domain1', 1)\n",
    "df = df.drop('rater2_domain1', 1)\n",
    "df = df.dropna(axis = 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble! Thing about! Dont you think so? How would you feel if your teenager is always on the phone with friends! Do you ever time to chat with your friends or buisness partner about things. Well now - there's a new way to chat the computer, theirs plenty of sites on the internet to do so: @ORGANIZATION1, @ORGANIZATION2, @CAPS1, facebook, myspace ect. Just think now while your setting up meeting with your boss on the computer, your teenager is having fun on the phone not rushing to get off cause you want to use it. How did you learn about other countrys/states outside of yours? Well I have by computer/internet, it's a new way to learn about what going on in our time! You might think your child spends a lot of time on the computer, but ask them so question about the economy, sea floor spreading or even about the @DATE1's you'll be surprise at how much he/she knows. Believe it or not the computer is much interesting then in class all day reading out of books. If your child is home on your computer or at a local library, it's better than being out with friends being fresh, or being perpressured to doing something they know isnt right. You might not know where your child is, @CAPS2 forbidde in a hospital bed because of a drive-by. Rather than your child on the computer learning, chatting or just playing games, safe and sound in your home or community place. Now I hope you have reached a point to understand and agree with me, because computers can have great effects on you or child because it gives us time to chat with friends/new people, helps us learn about the globe and believe or not keeps us out of troble. Thank you for listening.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays = df['essay']\n",
    "essays[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that personally identifiying information has been replaces with @NER where NER is a NER tag. We can remove these symbols to avoid interferring with the spell checking counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52 ms, sys: 36 ms, total: 88 ms\n",
      "Wall time: 109 ms\n"
     ]
    }
   ],
   "source": [
    "# Function to get all text from each essay - to build doc2vec\n",
    "def all_essays(df):\n",
    "    for (i, essay) in enumerate(df['essay']):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(essay), [i])\n",
    "        \n",
    "\n",
    "all_essay_lst = all_essays(df)\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(all_essay_lst)\n",
    "%time model.train(all_essay_lst, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features #TODO write description of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup pre-trained word2vec model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:51: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:54: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:58: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:61: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:64: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:67: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:70: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:74: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:75: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:76: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:77: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.shape[0]\n",
    "essays = df['essay'].values\n",
    "\n",
    "#initialize dataframe columns\n",
    "df['word_count'] = np.nan \n",
    "df['sentence_count'] = np.nan\n",
    "df['avg_word_length'] = np.nan \n",
    "df['num_exclamation_marks'] = np.nan\n",
    "df['num_question_marks'] = np.nan\n",
    "df['num_stopwords'] = np.nan\n",
    "df['word2vec_concat'] = np.nan\n",
    "\n",
    "df['verb_count'] = np.nan\n",
    "df['foreign_count'] = np.nan\n",
    "df['adj_count'] = np.nan\n",
    "df['conj_count'] = np.nan\n",
    "\n",
    "\n",
    "# TODO\n",
    "#df['num_advanced_words'] = np.nan\n",
    "#df['spelling_errors'] = np.nan\n",
    "\n",
    "def replace_punc(text):\n",
    "    return text.replace(\"@\", \"\").replace(\"%\", \"\")\n",
    "\n",
    "\n",
    "def get_pos_tags(essay):\n",
    "    nouns = verbs = foreign = adj = adv = conj = 0\n",
    "    tokens = nltk.word_tokenize(essay)\n",
    "    for token in tokens:\n",
    "        pos_tag = nltk.pos_tag(nltk.word_tokenize(token))\n",
    "        for (_, tag) in (pos_tag):\n",
    "            if tag[0] == \"N\":\n",
    "                nouns += 1\n",
    "            elif tag[0] == \"V\":\n",
    "                verbs += 1\n",
    "            elif tag[0:2] == \"FW\":\n",
    "                foreign += 1\n",
    "            elif tag[0] == \"J\":\n",
    "                adj += 1\n",
    "            elif tag[0] == \"R\":\n",
    "                adv += 1\n",
    "            elif tag[0:2] == \"CC\" or tag[0:2] == \"IN\":\n",
    "                conj += 1\n",
    "    \n",
    "    return [nouns, verbs, foreign, adj, adv, conj]\n",
    "\n",
    "\n",
    "for i in range(num_rows):\n",
    "    \n",
    "    # Remove placeholders\n",
    "    text = replace_punc(text)\n",
    "    \n",
    "    # Turn essay into list of words\n",
    "    text = essays[i].split(\" \")\n",
    "    \n",
    "    # Set word count\n",
    "    df.set_value(i,'word_count', len(text))\n",
    "    \n",
    "    # Sentence count\n",
    "    df.set_value(i, 'sentence_count', len(nltk.tokenize.sent_tokenize(essays[i])))\n",
    "    \n",
    "    # Average word length\n",
    "    word_len = sum(len(word) for word in text) / len(text)\n",
    "    df.set_value(i, 'avg_word_length', word_len)\n",
    "    \n",
    "    # Number of exclamation marks\n",
    "    df.set_value(i, \"num_exclamation_marks\", sum(word.count(\"!\") for word in essays[i]))\n",
    "    \n",
    "    # Number of question marks\n",
    "    df.set_value(i, \"num_question_marks\", sum(word.count(\"?\") for word in essays[i]))\n",
    "    \n",
    "    # Number of stop words\n",
    "    df.set_value(i, \"num_stopwords\", sum([1 for word in text if word.lower() in stopwords]))\n",
    "\n",
    "    # Word2Vec conversion - min + max\n",
    "    df.set_value(i, 'word2vec_concat', min(model.docvecs[i]) + max(model.docvecs[i]))\n",
    "    \n",
    "    # POS tag counts\n",
    "    pos_lst = get_pos_tags(essays[i])\n",
    "    df.set_value(i,'verb_count', pos_lst[1])\n",
    "    df.set_value(i,'foreign_count', pos_lst[2])\n",
    "    df.set_value(i,'adj_count', pos_lst[3])\n",
    "    df.set_value(i,'conj_count', pos_lst[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>...</th>\n",
       "      <th>state</th>\n",
       "      <th>story</th>\n",
       "      <th>thing</th>\n",
       "      <th>things</th>\n",
       "      <th>think</th>\n",
       "      <th>time</th>\n",
       "      <th>use</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "      <td>338.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.550296</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234392</td>\n",
       "      <td>0.202706</td>\n",
       "      <td>0.193065</td>\n",
       "      <td>0.180893</td>\n",
       "      <td>0.247159</td>\n",
       "      <td>0.225527</td>\n",
       "      <td>0.196461</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "      <td>419.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.463007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "      <td>279.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217546</td>\n",
       "      <td>0.203830</td>\n",
       "      <td>0.278498</td>\n",
       "      <td>0.254123</td>\n",
       "      <td>0.221372</td>\n",
       "      <td>0.289028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "      <td>524.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.041985</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "      <td>465.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240730</td>\n",
       "      <td>0.209705</td>\n",
       "      <td>0.273795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 77512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  word_count  sentence_count  avg_word_length  \\\n",
       "0              8       338.0            16.0         4.550296   \n",
       "1              9       419.0            20.0         4.463007   \n",
       "2              7       279.0            14.0         4.526882   \n",
       "3             10       524.0            27.0         5.041985   \n",
       "4              8       465.0            30.0         4.526882   \n",
       "\n",
       "   num_exclamation_marks  num_question_marks  num_stopwords    ...     state  \\\n",
       "0                    4.0                 2.0          168.0    ...       0.0   \n",
       "1                    1.0                 1.0          189.0    ...       0.0   \n",
       "2                    0.0                 0.0          140.0    ...       0.0   \n",
       "3                    2.0                 1.0          222.0    ...       0.0   \n",
       "4                    0.0                 0.0          236.0    ...       0.0   \n",
       "\n",
       "   story     thing    things     think      time       use      want  \\\n",
       "0    0.0  0.234392  0.202706  0.193065  0.180893  0.247159  0.225527   \n",
       "1    0.0  0.000000  0.000000  0.000000  0.000000  0.259524  0.000000   \n",
       "2    0.0  0.000000  0.000000  0.217546  0.203830  0.278498  0.254123   \n",
       "3    0.0  0.000000  0.000000  0.000000  0.000000  0.258546  0.000000   \n",
       "4    0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.240730   \n",
       "\n",
       "        way     world  \n",
       "0  0.196461  0.000000  \n",
       "1  0.000000  0.269336  \n",
       "2  0.221372  0.289028  \n",
       "3  0.000000  0.268321  \n",
       "4  0.209705  0.273795  \n",
       "\n",
       "[5 rows x 77512 columns]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tfidf_vectors(essays):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_df=0.9, min_df=5, max_features=50, stop_words=\"english\", binary=True)\n",
    "    tfidf_vectors = vectorizer.fit_transform(essays)\n",
    "    new_df = pd.DataFrame(tfidf_vectors.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    return pd.concat([df, new_df], axis=1)\n",
    "\n",
    "df = get_tfidf_vectors(essays)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text data\n",
    "\n",
    "We need to convert the essay strings into some numerical form. We could use Word2Vec, a TF-IDF Vectorizer, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>...</th>\n",
       "      <th>state</th>\n",
       "      <th>story</th>\n",
       "      <th>thing</th>\n",
       "      <th>things</th>\n",
       "      <th>think</th>\n",
       "      <th>time</th>\n",
       "      <th>use</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "      <td>0.248587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12977 rows × 77510 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                              essay  \\\n",
       "0             1          1  Dear local newspaper, I think effects computer...   \n",
       "1             2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2             3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3             4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4             5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "5             6          1  Dear @LOCATION1, I think that computers have a...   \n",
       "6             7          1  Did you know that more and more people these d...   \n",
       "7             8          1  @PERCENT1 of people agree that computers make ...   \n",
       "8             9          1  Dear reader, @ORGANIZATION1 has had a dramatic...   \n",
       "9            10          1  In the @LOCATION1 we have the technology of a ...   \n",
       "10           11          1  Dear @LOCATION1, @CAPS1 people acknowledge the...   \n",
       "11           12          1  Dear @CAPS1 @CAPS2 I feel that computers do ta...   \n",
       "12           13          1  Dear local newspaper I raed ur argument on the...   \n",
       "13           14          1  My three detaileds for this news paper article...   \n",
       "14           15          1  Dear, In this world today we should have every...   \n",
       "15           16          1  Dear @ORGANIZATION1, The computer blinked to l...   \n",
       "16           17          1  Dear Local Newspaper, I belive that computers ...   \n",
       "17           18          1  Dear Local Newspaper, I must admit that the ex...   \n",
       "18           19          1  I aegre waf the evansmant ov tnachnolage. The ...   \n",
       "19           20          1  Well computers can be a good or a bad thing. I...   \n",
       "20           21          1  Dear @CAPS1 of the @CAPS2 @CAPS3 daily, I am w...   \n",
       "21           22          1  Dear local Newspaper @CAPS1 a take all your co...   \n",
       "22           23          1  Dear local newspaper, @CAPS1 you ever see a ch...   \n",
       "23           24          1  Dear local newspaper, I've heard that not many...   \n",
       "24           25          1  Dear @CAPS1, @CAPS2 off, I beileve that comput...   \n",
       "25           26          1  Do you think that computers are useless? Or do...   \n",
       "26           27          1  Computers a good because you can get infermati...   \n",
       "27           28          1  Dear Newspaper, Computers are high tec and hav...   \n",
       "28           29          1  Dear local newspaper, @CAPS1 people throughout...   \n",
       "29           30          1  Dear Newspaper People, I think that computers ...   \n",
       "...         ...        ...                                                ...   \n",
       "12947     21592          8   We all understand the benefits of laughter. L...   \n",
       "12948     21594          8        It was midsummer, and i could feel the c...   \n",
       "12949     21595          8   Have you ever experienced a time with your fr...   \n",
       "12950     21596          8   I woke up just like any other day happy yet l...   \n",
       "12951     21598          8   Laughter is an important part of my life, eit...   \n",
       "12952     21599          8   I sat at the table, speechless, as they told ...   \n",
       "12953     21601          8   As I remember back, it was @DATE1. It was a h...   \n",
       "12954     21603          8   Those eyes, it was like I was looking out int...   \n",
       "12955     21604          8  Some say that laugh is the common language bet...   \n",
       "12956     21605          8   Laughter is an integral element to many situa...   \n",
       "12957     21606          8  One time I was at my friend @PERSON1's house, ...   \n",
       "12958     21607          8   LAUGHTER @CAPS1 knows that laughter is a heal...   \n",
       "12959     21608          8  One thing that people in the world love to do ...   \n",
       "12960     21609          8   Laughter, to me, is an important aspect of my...   \n",
       "12961     21610          8   People always say that the worst parts of lif...   \n",
       "12962     21611          8   Why is it that people can look back at someth...   \n",
       "12963     21613          8   Before my best friend moved away, we would st...   \n",
       "12964     21615          8                                @ORGANIZATION1  ...   \n",
       "12965     21617          8   Morose and somnolent, I woke up. I woke up to...   \n",
       "12966     21618          8   A while back my mom had decided to send me to...   \n",
       "12967     21619          8                              I dont like computers   \n",
       "12968     21620          8   Everyone knows how important a laugh can be. ...   \n",
       "12969     21621          8   Laughter is an important part of my family. W...   \n",
       "12970     21623          8   laughter is an important part of any kind of ...   \n",
       "12971     21624          8  Sometime ago on a hot @DATE1 day my @NUM1 ,@PE...   \n",
       "12972     21626          8   In most stories mothers and daughters are eit...   \n",
       "12973     21628          8   I never understood the meaning laughter is th...   \n",
       "12974     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
       "12975     21630          8                                 Trippin' on fen...   \n",
       "12976     21633          8   Many people believe that laughter can improve...   \n",
       "\n",
       "       domain1_score  word_count  sentence_count  avg_word_length  \\\n",
       "0                  8       338.0            16.0         4.550296   \n",
       "1                  9       419.0            20.0         4.463007   \n",
       "2                  7       279.0            14.0         4.526882   \n",
       "3                 10       524.0            27.0         5.041985   \n",
       "4                  8       465.0            30.0         4.526882   \n",
       "5                  8       246.0            15.0         4.191057   \n",
       "6                 10       499.0            30.0         4.629259   \n",
       "7                 10       482.0            39.0         4.653527   \n",
       "8                  9       443.0            35.0         4.424379   \n",
       "9                  9       502.0            26.0         4.245020   \n",
       "10                 8       325.0            22.0         5.043077   \n",
       "11                 8       392.0            25.0         4.568878   \n",
       "12                 7       204.0             6.0         3.941176   \n",
       "13                 6       308.0            25.0         4.256494   \n",
       "14                 6       176.0            13.0         4.636364   \n",
       "15                12       528.0            35.0         5.001894   \n",
       "16                 8       335.0            18.0         4.388060   \n",
       "17                 8       366.0            15.0         4.355191   \n",
       "18                 4        66.0             7.0         4.500000   \n",
       "19                 6       156.0            11.0         4.570513   \n",
       "20                 8       367.0            20.0         4.585831   \n",
       "21                 3        56.0             2.0         4.678571   \n",
       "22                10       521.0            30.0         4.525912   \n",
       "23                11       559.0            39.0         4.754919   \n",
       "24                 8       293.0            16.0         4.713311   \n",
       "25                 9       360.0            22.0         4.569444   \n",
       "26                 4       120.0             7.0         4.425000   \n",
       "27                 9       360.0            28.0         4.686111   \n",
       "28                 9       371.0            23.0         4.560647   \n",
       "29                 8       257.0            15.0         4.533074   \n",
       "...              ...         ...             ...              ...   \n",
       "12947             40       477.0            33.0         4.291405   \n",
       "12948             32       623.0            40.0         3.736758   \n",
       "12949             36       388.0            22.0         4.100515   \n",
       "12950             31       361.0            14.0         3.958449   \n",
       "12951             30       367.0            18.0         4.207084   \n",
       "12952             47       852.0            66.0         4.157277   \n",
       "12953             40       430.0            27.0         4.183721   \n",
       "12954             35       475.0            27.0         4.006316   \n",
       "12955             33       494.0            35.0         4.516194   \n",
       "12956             36       306.0            15.0         4.555556   \n",
       "12957             36       430.0            25.0         4.072093   \n",
       "12958             48       823.0            58.0         4.442284   \n",
       "12959             40       559.0            26.0         4.368515   \n",
       "12960             40       666.0            37.0         4.478979   \n",
       "12961             40       852.0            40.0         4.142019   \n",
       "12962             42       853.0            57.0         4.475967   \n",
       "12963             40       760.0            38.0         4.263158   \n",
       "12964             32       833.0            15.0         3.807923   \n",
       "12965             36       712.0            46.0         4.164326   \n",
       "12966             40       723.0            40.0         4.026279   \n",
       "12967             10         4.0             1.0         4.500000   \n",
       "12968             33       792.0            31.0         4.060606   \n",
       "12969             44       837.0            45.0         4.379928   \n",
       "12970             35       769.0            32.0         3.924577   \n",
       "12971             30       818.0             9.0         3.997555   \n",
       "12972             35       848.0            27.0         4.259434   \n",
       "12973             32       548.0            35.0         4.140511   \n",
       "12974             40       818.0            41.0         4.621027   \n",
       "12975             40       594.0            39.0         4.195286   \n",
       "12976             40       468.0            29.0         4.399573   \n",
       "\n",
       "       num_exclamation_marks  num_question_marks  num_stopwords    ...     \\\n",
       "0                        4.0                 2.0          168.0    ...      \n",
       "1                        1.0                 1.0          189.0    ...      \n",
       "2                        0.0                 0.0          140.0    ...      \n",
       "3                        2.0                 1.0          222.0    ...      \n",
       "4                        0.0                 0.0          236.0    ...      \n",
       "5                        1.0                 2.0          132.0    ...      \n",
       "6                        0.0                 4.0          237.0    ...      \n",
       "7                        1.0                 3.0          231.0    ...      \n",
       "8                        1.0                 6.0          218.0    ...      \n",
       "9                        0.0                 2.0          268.0    ...      \n",
       "10                       1.0                 1.0          139.0    ...      \n",
       "11                       0.0                 0.0          196.0    ...      \n",
       "12                       0.0                 0.0          105.0    ...      \n",
       "13                       0.0                 0.0          134.0    ...      \n",
       "14                       0.0                 0.0           75.0    ...      \n",
       "15                       0.0                 1.0          215.0    ...      \n",
       "16                       0.0                 1.0          184.0    ...      \n",
       "17                       0.0                 0.0          192.0    ...      \n",
       "18                       0.0                 0.0           23.0    ...      \n",
       "19                       2.0                 0.0           64.0    ...      \n",
       "20                       0.0                 1.0          177.0    ...      \n",
       "21                       0.0                 0.0           25.0    ...      \n",
       "22                       0.0                 2.0          254.0    ...      \n",
       "23                      10.0                 1.0          245.0    ...      \n",
       "24                       0.0                 0.0          143.0    ...      \n",
       "25                       8.0                 2.0          172.0    ...      \n",
       "26                       0.0                 0.0           60.0    ...      \n",
       "27                       2.0                 0.0          153.0    ...      \n",
       "28                       2.0                 1.0          189.0    ...      \n",
       "29                       0.0                 0.0          117.0    ...      \n",
       "...                      ...                 ...            ...    ...      \n",
       "12947                    0.0                 1.0          237.0    ...      \n",
       "12948                    0.0                 0.0          317.0    ...      \n",
       "12949                    0.0                 1.0          196.0    ...      \n",
       "12950                    0.0                 1.0          183.0    ...      \n",
       "12951                    0.0                 0.0          180.0    ...      \n",
       "12952                    0.0                 0.0          436.0    ...      \n",
       "12953                    0.0                 0.0          216.0    ...      \n",
       "12954                    0.0                 3.0          247.0    ...      \n",
       "12955                    8.0                 1.0          255.0    ...      \n",
       "12956                    0.0                 0.0          162.0    ...      \n",
       "12957                    2.0                 0.0          243.0    ...      \n",
       "12958                    0.0                 0.0          417.0    ...      \n",
       "12959                    0.0                 1.0          280.0    ...      \n",
       "12960                    0.0                 0.0          338.0    ...      \n",
       "12961                    2.0                 1.0          436.0    ...      \n",
       "12962                    4.0                 1.0          421.0    ...      \n",
       "12963                    0.0                 0.0          382.0    ...      \n",
       "12964                    1.0                 1.0          411.0    ...      \n",
       "12965                    0.0                 0.0          397.0    ...      \n",
       "12966                    0.0                 0.0          378.0    ...      \n",
       "12967                    0.0                 0.0            1.0    ...      \n",
       "12968                    0.0                 0.0          425.0    ...      \n",
       "12969                    2.0                 0.0          384.0    ...      \n",
       "12970                    3.0                 0.0          420.0    ...      \n",
       "12971                    0.0                 0.0          422.0    ...      \n",
       "12972                    0.0                 0.0          403.0    ...      \n",
       "12973                    0.0                10.0          257.0    ...      \n",
       "12974                    9.0                 7.0          399.0    ...      \n",
       "12975                    0.0                 2.0          280.0    ...      \n",
       "12976                    0.0                 0.0          239.0    ...      \n",
       "\n",
       "          state     story     thing    things     think      time       use  \\\n",
       "0      0.000000  0.000000  0.234392  0.202706  0.193065  0.180893  0.247159   \n",
       "1      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.259524   \n",
       "2      0.000000  0.000000  0.000000  0.000000  0.217546  0.203830  0.278498   \n",
       "3      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.258546   \n",
       "4      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5      0.000000  0.000000  0.271687  0.000000  0.223785  0.000000  0.000000   \n",
       "6      0.000000  0.000000  0.000000  0.000000  0.000000  0.197179  0.269410   \n",
       "7      0.000000  0.000000  0.000000  0.176904  0.168491  0.000000  0.000000   \n",
       "8      0.000000  0.000000  0.000000  0.219327  0.208896  0.000000  0.000000   \n",
       "9      0.000000  0.000000  0.214481  0.000000  0.176664  0.165526  0.000000   \n",
       "10     0.000000  0.000000  0.000000  0.000000  0.000000  0.203013  0.277382   \n",
       "11     0.000000  0.000000  0.240687  0.000000  0.000000  0.185751  0.253797   \n",
       "12     0.000000  0.000000  0.312461  0.000000  0.257370  0.000000  0.329481   \n",
       "13     0.285385  0.000000  0.267880  0.231666  0.220649  0.206738  0.282471   \n",
       "14     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.365707   \n",
       "15     0.000000  0.214001  0.230317  0.199181  0.189709  0.000000  0.242862   \n",
       "16     0.000000  0.000000  0.000000  0.290453  0.000000  0.259199  0.000000   \n",
       "17     0.000000  0.000000  0.000000  0.189101  0.000000  0.168753  0.230571   \n",
       "18     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "19     0.000000  0.000000  0.253514  0.219242  0.208815  0.000000  0.267322   \n",
       "20     0.000000  0.000000  0.000000  0.215206  0.000000  0.192049  0.262401   \n",
       "21     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "22     0.000000  0.000000  0.000000  0.216495  0.206199  0.193199  0.263973   \n",
       "23     0.000000  0.000000  0.000000  0.163461  0.155687  0.145871  0.000000   \n",
       "24     0.000000  0.000000  0.287685  0.000000  0.236962  0.222022  0.000000   \n",
       "25     0.286849  0.000000  0.000000  0.232854  0.221780  0.000000  0.283920   \n",
       "26     0.000000  0.000000  0.000000  0.322500  0.000000  0.287797  0.000000   \n",
       "27     0.000000  0.000000  0.245845  0.212610  0.000000  0.189732  0.000000   \n",
       "28     0.000000  0.000000  0.000000  0.000000  0.213441  0.199984  0.273244   \n",
       "29     0.000000  0.000000  0.000000  0.225936  0.215191  0.201624  0.275484   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "12947  0.000000  0.000000  0.238343  0.206123  0.000000  0.000000  0.000000   \n",
       "12948  0.000000  0.000000  0.229452  0.000000  0.188996  0.177081  0.000000   \n",
       "12949  0.000000  0.000000  0.000000  0.000000  0.000000  0.184896  0.000000   \n",
       "12950  0.000000  0.000000  0.319602  0.000000  0.000000  0.246654  0.000000   \n",
       "12951  0.289193  0.000000  0.271454  0.000000  0.000000  0.209496  0.000000   \n",
       "12952  0.000000  0.221523  0.238412  0.206182  0.000000  0.183996  0.000000   \n",
       "12953  0.000000  0.000000  0.249921  0.000000  0.000000  0.192877  0.263534   \n",
       "12954  0.000000  0.000000  0.000000  0.000000  0.227401  0.000000  0.000000   \n",
       "12955  0.000000  0.205096  0.220732  0.000000  0.181814  0.170351  0.000000   \n",
       "12956  0.000000  0.285013  0.000000  0.000000  0.000000  0.236730  0.000000   \n",
       "12957  0.000000  0.291569  0.313798  0.000000  0.000000  0.242175  0.000000   \n",
       "12958  0.000000  0.000000  0.000000  0.000000  0.000000  0.176542  0.000000   \n",
       "12959  0.000000  0.000000  0.219969  0.190232  0.181185  0.000000  0.231951   \n",
       "12960  0.000000  0.000000  0.000000  0.000000  0.000000  0.185390  0.000000   \n",
       "12961  0.223783  0.000000  0.000000  0.181660  0.000000  0.162112  0.000000   \n",
       "12962  0.227646  0.000000  0.213682  0.184795  0.000000  0.164910  0.000000   \n",
       "12963  0.195431  0.000000  0.183444  0.158645  0.000000  0.141574  0.000000   \n",
       "12964  0.000000  0.220165  0.000000  0.000000  0.000000  0.182867  0.000000   \n",
       "12965  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12966  0.000000  0.000000  0.237703  0.205568  0.195792  0.183448  0.000000   \n",
       "12967  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12968  0.000000  0.196582  0.211569  0.000000  0.174266  0.163279  0.000000   \n",
       "12969  0.000000  0.183618  0.197617  0.000000  0.162774  0.152512  0.208381   \n",
       "12970  0.000000  0.000000  0.230942  0.000000  0.190223  0.178230  0.000000   \n",
       "12971  0.000000  0.000000  0.000000  0.000000  0.000000  0.184710  0.000000   \n",
       "12972  0.000000  0.184362  0.198418  0.171594  0.163434  0.153130  0.209225   \n",
       "12973  0.000000  0.000000  0.209758  0.181401  0.172774  0.161881  0.000000   \n",
       "12974  0.000000  0.000000  0.224127  0.193828  0.000000  0.172971  0.000000   \n",
       "12975  0.000000  0.000000  0.000000  0.224555  0.000000  0.200392  0.000000   \n",
       "12976  0.000000  0.000000  0.000000  0.000000  0.187106  0.175310  0.000000   \n",
       "\n",
       "           want       way     world  \n",
       "0      0.225527  0.196461  0.000000  \n",
       "1      0.000000  0.000000  0.269336  \n",
       "2      0.254123  0.221372  0.289028  \n",
       "3      0.000000  0.000000  0.268321  \n",
       "4      0.240730  0.209705  0.273795  \n",
       "5      0.000000  0.227721  0.000000  \n",
       "6      0.000000  0.000000  0.279596  \n",
       "7      0.196820  0.171454  0.223855  \n",
       "8      0.244020  0.212571  0.000000  \n",
       "9      0.206368  0.179772  0.000000  \n",
       "10     0.000000  0.000000  0.287869  \n",
       "11     0.000000  0.000000  0.263393  \n",
       "12     0.300643  0.000000  0.341938  \n",
       "13     0.000000  0.000000  0.000000  \n",
       "14     0.000000  0.290692  0.379534  \n",
       "15     0.221606  0.000000  0.252044  \n",
       "16     0.323154  0.000000  0.000000  \n",
       "17     0.000000  0.183276  0.000000  \n",
       "18     0.000000  0.000000  0.000000  \n",
       "19     0.000000  0.000000  0.277429  \n",
       "20     0.000000  0.000000  0.000000  \n",
       "21     0.000000  0.000000  0.534095  \n",
       "22     0.000000  0.000000  0.000000  \n",
       "23     0.181864  0.158425  0.206844  \n",
       "24     0.000000  0.000000  0.000000  \n",
       "25     0.259070  0.000000  0.000000  \n",
       "26     0.358808  0.000000  0.000000  \n",
       "27     0.236547  0.000000  0.000000  \n",
       "28     0.000000  0.000000  0.283574  \n",
       "29     0.251373  0.000000  0.000000  \n",
       "...         ...       ...       ...  \n",
       "12947  0.000000  0.199773  0.000000  \n",
       "12948  0.000000  0.192320  0.251098  \n",
       "12949  0.230518  0.000000  0.262180  \n",
       "12950  0.000000  0.000000  0.000000  \n",
       "12951  0.000000  0.227525  0.000000  \n",
       "12952  0.229395  0.199830  0.000000  \n",
       "12953  0.000000  0.000000  0.000000  \n",
       "12954  0.000000  0.231400  0.302121  \n",
       "12955  0.000000  0.000000  0.241555  \n",
       "12956  0.000000  0.257103  0.000000  \n",
       "12957  0.000000  0.263017  0.343401  \n",
       "12958  0.000000  0.191735  0.000000  \n",
       "12959  0.211649  0.000000  0.240720  \n",
       "12960  0.000000  0.000000  0.262881  \n",
       "12961  0.202112  0.176064  0.229873  \n",
       "12962  0.000000  0.179103  0.000000  \n",
       "12963  0.176505  0.153757  0.200749  \n",
       "12964  0.000000  0.198605  0.000000  \n",
       "12965  0.000000  0.000000  0.000000  \n",
       "12966  0.000000  0.199236  0.000000  \n",
       "12967  0.000000  0.000000  0.000000  \n",
       "12968  0.000000  0.177331  0.000000  \n",
       "12969  0.190143  0.000000  0.000000  \n",
       "12970  0.000000  0.193569  0.000000  \n",
       "12971  0.000000  0.200606  0.000000  \n",
       "12972  0.000000  0.166308  0.000000  \n",
       "12973  0.000000  0.000000  0.000000  \n",
       "12974  0.000000  0.187857  0.000000  \n",
       "12975  0.000000  0.217638  0.000000  \n",
       "12976  0.000000  0.190397  0.248587  \n",
       "\n",
       "[12977 rows x 77510 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['word2vec_min', 'word2vec_max'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have added all the features, so now we can start to explore correlations between features and scores (to ensure we are making correct assumptions and to discover potential new features), and perform the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''         \n",
    "Worsened: 'word2vec_avg', 'noun_count','adj_count', 'adv_count', 'foreign_count',\n",
    "'''\n",
    "\n",
    "features = df.drop('domain1_score', axis=1)\n",
    "x = df[features]\n",
    "y = df['domain1_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
    "logistic_reg = LogisticRegression()\n",
    "logistic_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression classifier accuracy: 0.41\n"
     ]
    }
   ],
   "source": [
    "predictions = logistic_reg.predict(X_test)\n",
    "print('Logistic regression classifier accuracy: {:.2f}'.format(logistic_reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4295056336869677\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(predictions, y_test, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm = textmining.TermDocumentMatrix()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
