{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Essay Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "Automated essay scoring requires quantifying not only grammar but semantics, discourse and pragmatics. Two different approaches were explored - traditional NLP features with logistic regression, and word vector representations with a LSTM. Because the contest was held seven years ago, the contest entries all involved feature engineering and making use of different regressors such as linear regression and k-nearest neighbours. Recent attempts at the contest involve neural models, but not much work has been done in this area. Trying out the two different techniques allowed for a better learning experience with regards to finding which traditional NLP features correlate most to the final score, as well as providing the opportunity to build upon the current state of the art neural model and work towards beating it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "We explored classical natural language techniques by designing handcrafted features and performing logistic regression, and experimented with different word vector techniques with a LSTM. \n",
    "\n",
    "## Features\n",
    "\n",
    "Features were designed to judge language fluidity, diction, structure, organization, originality and quality of the content. The selected features were as follows.  \n",
    "\n",
    "1.Language quality and originality.  \n",
    "\n",
    "- TF-IDF vectors: A TF-IDF vectorizer was trained on the essays and 400 features were selected as unigrams, bigrams, or trigrams. We ensured that each n-gram was observed at least five times in the essay but occurred in no more than 90% of the essays. Then, each n-gram was fed as a binary feature with a weight of one if it appeared, and zero otherwise.\n",
    " \n",
    "- Doc2Vec: A Doc2Vec model was built from the essays, and a concatenation of the maximum and minimum vectors for each essay was fed as a feature. This allows us to encode semantic meaning from the essays, and concatenation performed better than summing or averaging the vectors. \n",
    "\n",
    "2.Numerical features. \n",
    "- Basic text features: Word count, average word length, and sentence count. \n",
    "- Part of speech counts: Number of nouns, verbs, foreign words, adjectives, adverbs, and conjunctions.\n",
    "\n",
    "3.Structure and organization. \n",
    "- Punctuation: Number of exclamation marks and question marks.\n",
    "\n",
    "## Logistic Regression \n",
    "\n",
    "Logistic regression was used as the learning model to make predictions based on the features. 5-fold cross validation was used in training and testing the model to avoid over-fitting.\n",
    "\n",
    "## Long Short-Term Memory\n",
    "\n",
    "Long short-term memory units are a modification to recurrent units that use three gates to forget information or preserve it. The model consists of two LSTM layers, a dropout layer, and a dense output layer. The dropout rate was set to 50% to guard against over-fitting. \n",
    "${W}$ vectors represent the weight for the input vectors, ${U}$ vectors are the weights for the previous cell output, ${x_t}$ is the input vector at time t, ${h_t}$ is the output vector at time t, and ${\\circ}$ represents element-wise multiplication.  \n",
    "\n",
    "\n",
    "The input gate is expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "i_t  =  \\sigma (W_i .x_t + U_i.h_{t- 1} + b_i)\n",
    "\\end{equation}\n",
    "\n",
    "The forget gate is expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "f_t  =  \\sigma (W_i .x_t + U_f.h_{t- 1} + b_f)\n",
    "\\end{equation}\n",
    "\n",
    "The output of the element-wise product of the previous state and the forget gate is ${S_{t- 1}} \\circ f $. Then the output is:\n",
    "\n",
    "\\begin{equation}\n",
    " s_t = s_{t-1} \\circ f + g  \\circ f\n",
    "\\end{equation}\n",
    "\n",
    "Lastly, the output gate:\n",
    "\n",
    "\\begin{equation}\n",
    "o_t  =  \\sigma (W_o .x_t + U_o.h_{t- 1} + b_o)\n",
    "\\end{equation}\n",
    "\n",
    "The final result is put through tanh squashing, as this ensures the range is from -1 to 1. This stage is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "h_t  =  \\tanh{o_t} \\circ {s_t}\n",
    "\\end{equation}\n",
    "\n",
    "The outputs of this final layer are then fed into a dense layer, a densely-connected layer. It implements output = activation(dot(input, weights)) where activation is the element-wise ReLu activation function and weights is a weights matrix created by the layer:\n",
    "\n",
    "\\begin{equation}\n",
    "d(h_t) = o_t \\circ ReLu((h_t .W_t))\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data set was provided from Hewlett Foundation’s Automated Student Assessment Prize competition on Kaggle. The dataset can be found in the Data folder, it is the file called 'training_set_rel3.xls'. Descriptions for each essay set are shown there as well.\n",
    "\t\n",
    "- 12977 essay samples \n",
    "- 80% of the essays were used for training, and 20% for testing \n",
    "- 8 different essay prompts, each of which have a corresponding set of essays \n",
    "- Each set has a unique grading scale \n",
    "- Average word length of 150-550 words per essay \n",
    "- 2 essay sets are argumentative, 4 are response essays, and 2 are narrative (source dependent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim \n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can set up the dataframes and explore the data. We will drop columns that we don't need and those with NaN values. There was one row without a domain1_score, which I removed. Some essays also contained domain2 or domain3 scores, but since not all the data has that field, we will ignore those fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0              8  \n",
       "1              9  \n",
       "2              7  \n",
       "3             10  \n",
       "4              8  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.ExcelFile('./data/training_set_rel3.xls')\n",
    "df = data.parse(\"training_set\")\n",
    "df = df.drop('rater1_domain1', 1)\n",
    "df = df.drop('rater2_domain1', 1)\n",
    "df = df.dropna(axis = 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble! Thing about! Dont you think so? How would you feel if your teenager is always on the phone with friends! Do you ever time to chat with your friends or buisness partner about things. Well now - there's a new way to chat the computer, theirs plenty of sites on the internet to do so: @ORGANIZATION1, @ORGANIZATION2, @CAPS1, facebook, myspace ect. Just think now while your setting up meeting with your boss on the computer, your teenager is having fun on the phone not rushing to get off cause you want to use it. How did you learn about other countrys/states outside of yours? Well I have by computer/internet, it's a new way to learn about what going on in our time! You might think your child spends a lot of time on the computer, but ask them so question about the economy, sea floor spreading or even about the @DATE1's you'll be surprise at how much he/she knows. Believe it or not the computer is much interesting then in class all day reading out of books. If your child is home on your computer or at a local library, it's better than being out with friends being fresh, or being perpressured to doing something they know isnt right. You might not know where your child is, @CAPS2 forbidde in a hospital bed because of a drive-by. Rather than your child on the computer learning, chatting or just playing games, safe and sound in your home or community place. Now I hope you have reached a point to understand and agree with me, because computers can have great effects on you or child because it gives us time to chat with friends/new people, helps us learn about the globe and believe or not keeps us out of troble. Thank you for listening.\""
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get an essay sample\n",
    "essays = df['essay']\n",
    "essays[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec Model\n",
    "\n",
    "Next, we need to convert the essays into vector representations. The following is from a [Gensim Doc2Vec tutorial](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb). This allows us to learn paragraph and document embeddings via the distributed memory and distributed bag of words models. \n",
    "\n",
    "We create a Doc2Vec model with a vector size with 50 words and iterate over the training data 40 times. The minimum word count is two in order to discard uncommon words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.6 ms, sys: 37.6 ms, total: 92.2 ms\n",
      "Wall time: 99.5 ms\n"
     ]
    }
   ],
   "source": [
    "# Function to get all text from each essay - to build doc2vec\n",
    "def all_essays(df):\n",
    "    for (i, essay) in enumerate(df['essay']):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(essay), [i])\n",
    "        \n",
    "\n",
    "all_essay_lst = all_essays(df)\n",
    "\n",
    "# Instaniate the Doc2Vec model\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "\n",
    "# Build dictionary of all the unique words and their frequencies\n",
    "model.build_vocab(all_essay_lst)\n",
    "\n",
    "%time model.train(all_essay_lst, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Features were designed to judge language fluidity, diction, structure, organization, originality and quality of the content. The selected features were as follows.  \n",
    "\n",
    "1.Language quality and originality.  \n",
    "\n",
    "- TF-IDF vectors: A TF-IDF vectorizer was trained on the essays and 400 features were selected as unigrams, bigrams, or trigrams. We ensured that each n-gram was observed at least five times in the essay but occurred in no more than 90% of the essays. Then, each n-gram was fed as a binary feature with a weight of one if it appeared, and zero otherwise.\n",
    " \n",
    "- Doc2Vec: A Doc2Vec model was built from the essays, and a concatenation of the maximum and minimum vectors for each essay was fed as a feature. This allows us to encode semantic meaning from the essays, and concatenation performed better than summing or averaging the vectors. \n",
    "\n",
    "2.Numerical features. \n",
    "- Basic text features: Word count, average word length, and sentence count. \n",
    "- Part of speech counts: Number of nouns, verbs, foreign words, adjectives, adverbs, and conjunctions.\n",
    "\n",
    "3.Structure and organization. \n",
    "- Punctuation: Number of exclamation marks and question marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:48: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:51: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:55: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:58: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:61: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:64: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:67: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:71: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:72: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:73: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:74: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:75: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:76: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.shape[0]\n",
    "essays = df['essay'].values\n",
    "\n",
    "#Initialize dataframe columns\n",
    "df['word_count'] = np.nan \n",
    "df['sentence_count'] = np.nan\n",
    "df['avg_word_length'] = np.nan \n",
    "df['num_exclamation_marks'] = np.nan\n",
    "df['num_question_marks'] = np.nan\n",
    "df['num_stopwords'] = np.nan\n",
    "df['word2vec_concat'] = np.nan\n",
    "\n",
    "df['noun_count'] = np.nan\n",
    "df['verb_count'] = np.nan\n",
    "df['foreign_count'] = np.nan\n",
    "df['adj_count'] = np.nan\n",
    "df['conj_count'] = np.nan\n",
    "df['adv_count'] = np.nan\n",
    "\n",
    "def get_pos_tags(essay):\n",
    "    nouns = verbs = foreign = adj = adv = conj = 0\n",
    "    tokens = nltk.word_tokenize(essay)\n",
    "    for token in tokens:\n",
    "        pos_tag = nltk.pos_tag(nltk.word_tokenize(token))\n",
    "        for (_, tag) in (pos_tag):\n",
    "            if tag[0] == \"N\":\n",
    "                nouns += 1\n",
    "            elif tag[0] == \"V\":\n",
    "                verbs += 1\n",
    "            elif tag[0:2] == \"FW\":\n",
    "                foreign += 1\n",
    "            elif tag[0] == \"J\":\n",
    "                adj += 1\n",
    "            elif tag[0] == \"R\":\n",
    "                adv += 1\n",
    "            elif tag[0:2] == \"CC\" or tag[0:2] == \"IN\":\n",
    "                conj += 1\n",
    "    \n",
    "    return [nouns, verbs, foreign, adj, adv, conj]\n",
    "\n",
    "\n",
    "for i in range(num_rows):\n",
    "    \n",
    "    # Turn essay into list of words\n",
    "    text = essays[i].split(\" \")\n",
    "    \n",
    "    # Set word count\n",
    "    df.set_value(i,'word_count', len(text))\n",
    "    \n",
    "    # Sentence count\n",
    "    df.set_value(i, 'sentence_count', len(nltk.tokenize.sent_tokenize(essays[i])))\n",
    "    \n",
    "    # Average word length\n",
    "    word_len = sum(len(word) for word in text) / len(text)\n",
    "    df.set_value(i, 'avg_word_length', word_len)\n",
    "    \n",
    "    # Number of exclamation marks\n",
    "    df.set_value(i, \"num_exclamation_marks\", sum(word.count(\"!\") for word in essays[i]))\n",
    "    \n",
    "    # Number of question marks\n",
    "    df.set_value(i, \"num_question_marks\", sum(word.count(\"?\") for word in essays[i]))\n",
    "    \n",
    "    # Number of stop words\n",
    "    df.set_value(i, \"num_stopwords\", sum([1 for word in text if word.lower() in stopwords]))\n",
    "\n",
    "    # Doc2Vec conversion - min + max\n",
    "    df.set_value(i, 'word2vec_concat', min(model.docvecs[i]) + max(model.docvecs[i]))\n",
    "    \n",
    "    # POS tag counts\n",
    "    pos_lst = get_pos_tags(essays[i])\n",
    "    df.set_value(i,'noun_count', pos_lst[0])\n",
    "    df.set_value(i,'verb_count', pos_lst[1])\n",
    "    df.set_value(i,'foreign_count', pos_lst[2])\n",
    "    df.set_value(i,'adj_count', pos_lst[3])\n",
    "    df.set_value(i,'adv_count', pos_lst[4])\n",
    "    df.set_value(i,'conj_count', pos_lst[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF vectors\n",
    "\n",
    "A TF-IDF vectorizer was trained on the essays and 400 features were selected as unigrams, bigrams, or trigrams. We ensured that each n-gram was observed at least five times in the essay but occurred in no more than 90% of the essays. Then, each n-gram was fed as a binary feature with a weight of one if it appeared, and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_vectors(essays):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_df=0.9, min_df=5, max_features=400, stop_words=\"english\", binary=True)\n",
    "    tfidf_vectors = vectorizer.fit_transform(essays)\n",
    "    new_df = pd.DataFrame(tfidf_vectors.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    return pd.concat([df, new_df], axis=1)\n",
    "\n",
    "df = get_tfidf_vectors(essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now view the updated training set that includes all of these new features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>wouldn</th>\n",
       "      <th>write</th>\n",
       "      <th>writing</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "      <td>338.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.550296</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "      <td>419.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.463007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121001</td>\n",
       "      <td>0.151485</td>\n",
       "      <td>0.173362</td>\n",
       "      <td>0.173767</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "      <td>279.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.15899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "      <td>524.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.041985</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "      <td>465.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116441</td>\n",
       "      <td>0.145776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 818 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  word_count  sentence_count  avg_word_length  \\\n",
       "0              8       338.0            16.0         4.550296   \n",
       "1              9       419.0            20.0         4.463007   \n",
       "2              7       279.0            14.0         4.526882   \n",
       "3             10       524.0            27.0         5.041985   \n",
       "4              8       465.0            30.0         4.526882   \n",
       "\n",
       "   num_exclamation_marks  num_question_marks  num_stopwords  ...       world  \\\n",
       "0                    4.0                 2.0          168.0  ...    0.000000   \n",
       "1                    1.0                 1.0          189.0  ...    0.121001   \n",
       "2                    0.0                 0.0          140.0  ...    0.125760   \n",
       "3                    2.0                 1.0          222.0  ...    0.111728   \n",
       "4                    0.0                 0.0          236.0  ...    0.116441   \n",
       "\n",
       "     wouldn     write   writing    wrong  year  years  yes  york  young  \n",
       "0  0.000000  0.000000  0.000000  0.00000   0.0    0.0  0.0   0.0    0.0  \n",
       "1  0.151485  0.173362  0.173767  0.00000   0.0    0.0  0.0   0.0    0.0  \n",
       "2  0.000000  0.000000  0.000000  0.15899   0.0    0.0  0.0   0.0    0.0  \n",
       "3  0.000000  0.160076  0.000000  0.00000   0.0    0.0  0.0   0.0    0.0  \n",
       "4  0.145776  0.000000  0.000000  0.00000   0.0    0.0  0.0   0.0    0.0  \n",
       "\n",
       "[5 rows x 818 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "We use sklearn for the logistic regression. First, we split up our dataset. We want to train on our new features without the score and essay removed, and test on the score column. Then, we can use 5 fold cross-validation to split up the two sets. Finally, we will train the logistic regression model on our training set.\n",
    "\n",
    "The linear equation is:\n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + {b_1}x \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Logistic regression is similar to linear regression but it produces a logistic curve, which is limited to values between 0 and 1. The curve is created by using the natural logarithm of the probabibility of the target variable, rather than the probability. The logistic equation is: \n",
    "\n",
    "\\begin{equation}\n",
    "p = \\frac{{1}}{e^{-(b_0 + {b_1}x)}}\n",
    "\\end{equation}\n",
    "\n",
    "The constant, $b_0$ moves the curve left and right and the slope, $b_1$ determines the steepness of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "x = df.drop(['domain1_score', 'essay'], axis=1)\n",
    "y = df['domain1_score']\n",
    "\n",
    "# 5 fold cross validation to avoid overfitting\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "for train_index, test_index in kfold.split(x, y):\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "logistic_reg = LogisticRegression()\n",
    "logistic_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can test our model on the unseen data and get an accuracy score and a quadratic weighted kappa score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression classifier accuracy: 0.5670423630003887\n"
     ]
    }
   ],
   "source": [
    "predictions = logistic_reg.predict(X_test)\n",
    "print('Logistic regression classifier accuracy:', logistic_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8472772087653904\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(predictions, y_test, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory Network\n",
    "\n",
    "Long short-term memory units are a modification to recurrent units that use three gates to forget information or preserve it. The model consists of two LSTM layers, a dropout layer, and a dense output layer. The dropout rate was set to 50% to guard against over-fitting.\n",
    "\n",
    "The first layer of the LSTM has 300 units, 40% of which are dropped for the linear transformation of the input, and 40% of which are dropped for the linear transformation of the recurrent state. The second layer has 64 units, and 40% of units are dropped for the linear transformation of the recurrent state. Then, it runs a Dropout layer, which randomly sets 50% of input units to 0 at each update during training as a way to reduce overfitting. Lastly, it goes to a Dense layer, which is a densely-connected layer. It implements output = activation(dot(input, weights)) where activation is the element-wise ReLu activation function and weights is a weights matrix created by the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "X = pd.ExcelFile('./data/training_set_rel3.xls')\n",
    "X = data.parse(\"training_set\")\n",
    "y = X['domain1_score']\n",
    "X = X.dropna(axis=1)\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "\n",
    "def get_model():\n",
    "    model = Sequential([\n",
    "        LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True),\n",
    "        LSTM(64, recurrent_dropout=0.4),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='relu')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def essay_to_list(essay):\n",
    "    # Remove the tags\n",
    "    essay = re.sub(\"[^a-zA-Z]\", \" \", essay)\n",
    "    words = essay.lower().split()\n",
    "    return [w for w in words if not w in stopwords]\n",
    "\n",
    "def essay_to_sentences(essay):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_list(raw_sentence))\n",
    "    return sentences\n",
    "\n",
    "# Generate feature vector for the words\n",
    "def get_feature_vector(words, model, num_features, vec_type=\"sum\"):\n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    max_vec =  np.zeros((num_features,),dtype=\"float32\")\n",
    "    min_vec =  np.ones((num_features,),dtype=\"float32\")\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            max_vec = np.maximum(model[word], feature_vector)\n",
    "            min_vec = np.minimum(model[word], feature_vector)\n",
    "            feature_vector = np.add(feature_vector, model[word]) \n",
    "    \n",
    "    # return min vector + max vector\n",
    "    if vec_type == \"min+max\":\n",
    "        return np.add(min_vec, max_vec) \n",
    "    \n",
    "    # average of vectors\n",
    "    elif vec_type == \"average\":\n",
    "        return np.divide(feature_vector, num_words)\n",
    "\n",
    "    # return sum of word2vec vectors\n",
    "    return feature_vector\n",
    "\n",
    "# Generate word vectors from the mdoel\n",
    "def generate_essay_vectors(essays, model, num_features, vec_type=\"sum\"):\n",
    "    essayfeature_vectors = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for (i, essay) in enumerate(essays):\n",
    "        essayfeature_vectors[i] = get_feature_vector(essay, model, num_features, vec_type)\n",
    "    return essayfeature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def train_model(X, y, dataset, vec_type=\"sum\"):\n",
    "    count = 1\n",
    "    results = []\n",
    "    \n",
    "    for train_set, test_set in dataset:\n",
    "        print(\"Fold #\", count)\n",
    "        X_test, X_train, y_test, y_train = X.iloc[test_set], X.iloc[train_set], y.iloc[test_set], y.iloc[train_set]\n",
    "        \n",
    "        train_essays = X_train['essay']\n",
    "        test_essays = X_test['essay']\n",
    "        \n",
    "        sentences = []\n",
    "        \n",
    "        for essay in train_essays:\n",
    "            sentences += essay_to_sentences(essay)\n",
    "                \n",
    "        # Initialize variables for word2vec model\n",
    "        num_features = 300 \n",
    "        min_word_count = 40\n",
    "        num_workers = 4\n",
    "        context = 10\n",
    "        downsampling = 1e-7\n",
    "\n",
    "        # Train the word2vec model\n",
    "        model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "        model.init_sims(replace=True)\n",
    "\n",
    "        clean_train_essays = []\n",
    "        \n",
    "        # Generate training and testing data word vectors.\n",
    "        for essay_vec in train_essays:\n",
    "            clean_train_essays.append(essay_to_list(essay_vec))\n",
    "        train_vectors = generate_essay_vectors(clean_train_essays, model, num_features, vec_type)\n",
    "        \n",
    "        clean_test_essays = []\n",
    "        for essay_vec in test_essays:\n",
    "            clean_test_essays.append(essay_to_list( essay_vec))\n",
    "        test_vectors = generate_essay_vectors(clean_test_essays, model, num_features, vec_type)\n",
    "        \n",
    "        train_vectors = np.array(train_vectors)\n",
    "        test_vectors = np.array(test_vectors)\n",
    "\n",
    "        # Reshape the train and test vectors to 3 dimensions - 1 represents one timestamp \n",
    "        train_vectors = np.reshape(train_vectors, (train_vectors.shape[0], 1, train_vectors.shape[1]))\n",
    "        test_vectors = np.reshape(test_vectors, (test_vectors.shape[0], 1, test_vectors.shape[1]))\n",
    "        \n",
    "        # Call the LSTM to get the score predictions \n",
    "        lstm_model = get_model()\n",
    "        lstm_model.fit(train_vectors, y_train, batch_size=64, epochs=50)\n",
    "        y_pred = lstm_model.predict(test_vectors)\n",
    "        \n",
    "        # Round the prediction to the nearest integer\n",
    "        y_pred = np.around(y_pred)\n",
    "        \n",
    "        # Evaluate the model: quadratic kappa score of predictions against human grading\n",
    "        result = cohen_kappa_score(y_test.values, y_pred, weights='quadratic')\n",
    "        print(\"QWK: \", result)\n",
    "        results.append(result)\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:37: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:38: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_124 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_125 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 90s 9ms/step - loss: 52.6787 - mean_absolute_error: 3.5893\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 22s 2ms/step - loss: 29.5040 - mean_absolute_error: 2.5688\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 23s 2ms/step - loss: 18.7404 - mean_absolute_error: 2.1375\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 22s 2ms/step - loss: 13.3977 - mean_absolute_error: 1.8606\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 9.5645 - mean_absolute_error: 1.6471\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 8.3612 - mean_absolute_error: 1.5708\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 7.5198 - mean_absolute_error: 1.4953\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 6.9804 - mean_absolute_error: 1.4486: 2s - loss: 7.0205\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 7.2982 - mean_absolute_error: 1.4593\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 6.6951 - mean_absolute_error: 1.4104\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 6.4657 - mean_absolute_error: 1.3974\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 6.3982 - mean_absolute_error: 1.3759\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 5.8504 - mean_absolute_error: 1.3329\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 5.6370 - mean_absolute_error: 1.3193\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 19s 2ms/step - loss: 5.9535 - mean_absolute_error: 1.3190\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 5.5640 - mean_absolute_error: 1.2884\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 5.4063 - mean_absolute_error: 1.2721\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 5.4028 - mean_absolute_error: 1.2674\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 5.4344 - mean_absolute_error: 1.2669\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 5.0487 - mean_absolute_error: 1.2269\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 5.3962 - mean_absolute_error: 1.2447\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 5.0792 - mean_absolute_error: 1.2207\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 4.9075 - mean_absolute_error: 1.2035\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 4.7768 - mean_absolute_error: 1.1779\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 4.7036 - mean_absolute_error: 1.1731\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 4.6301 - mean_absolute_error: 1.1655\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 4.6224 - mean_absolute_error: 1.1586\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 4.5673 - mean_absolute_error: 1.1458\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 21s 2ms/step - loss: 4.3672 - mean_absolute_error: 1.1268\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 20s 2ms/step - loss: 4.6383 - mean_absolute_error: 1.1422\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 19s 2ms/step - loss: 4.5356 - mean_absolute_error: 1.1502\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 20s 2ms/step - loss: 4.3626 - mean_absolute_error: 1.1157\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 23s 2ms/step - loss: 4.4175 - mean_absolute_error: 1.1187\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 4.2104 - mean_absolute_error: 1.1007\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 4.0006 - mean_absolute_error: 1.0866\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 3.9713 - mean_absolute_error: 1.0709\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 4.1821 - mean_absolute_error: 1.0921\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 4.2036 - mean_absolute_error: 1.0976\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 3.9879 - mean_absolute_error: 1.0717\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 16s 1ms/step - loss: 3.9899 - mean_absolute_error: 1.0719\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 3.7936 - mean_absolute_error: 1.0573\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 24s 2ms/step - loss: 3.7661 - mean_absolute_error: 1.0437\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 22s 2ms/step - loss: 3.8902 - mean_absolute_error: 1.0585\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 21s 2ms/step - loss: 3.8524 - mean_absolute_error: 1.0479\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 21s 2ms/step - loss: 3.9836 - mean_absolute_error: 1.0593\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 21s 2ms/step - loss: 3.9453 - mean_absolute_error: 1.0392\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 3.7220 - mean_absolute_error: 1.0351\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 3.8044 - mean_absolute_error: 1.0385\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 3.7402 - mean_absolute_error: 1.0306\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 21s 2ms/step - loss: 3.6602 - mean_absolute_error: 1.0295\n",
      "QWK:  0.9729001412331556\n",
      "Fold # 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_126 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_127 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 114s 11ms/step - loss: 52.8237 - mean_absolute_error: 3.5963\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 29.6137 - mean_absolute_error: 2.5833\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 18.5714 - mean_absolute_error: 2.1404\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 12.6647 - mean_absolute_error: 1.8316\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10381/10381 [==============================] - 20s 2ms/step - loss: 9.9089 - mean_absolute_error: 1.6749\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 8.5742 - mean_absolute_error: 1.5717\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 7.3449 - mean_absolute_error: 1.4921\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 7.2418 - mean_absolute_error: 1.4726\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 6.8984 - mean_absolute_error: 1.4201\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 6.7867 - mean_absolute_error: 1.4111\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 6.5055 - mean_absolute_error: 1.3931\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 6.1833 - mean_absolute_error: 1.3640\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 6.1524 - mean_absolute_error: 1.3372\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 5.6062 - mean_absolute_error: 1.3086\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 5.6710 - mean_absolute_error: 1.2970\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 5.7074 - mean_absolute_error: 1.2975\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 5.2897 - mean_absolute_error: 1.2586\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 5.3742 - mean_absolute_error: 1.2535\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 5.4532 - mean_absolute_error: 1.2515\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 4.9620 - mean_absolute_error: 1.2205\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.8069 - mean_absolute_error: 1.1993\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 5.1071 - mean_absolute_error: 1.2208\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.9126 - mean_absolute_error: 1.2108\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.8920 - mean_absolute_error: 1.2043\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.6747 - mean_absolute_error: 1.1810\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.5986 - mean_absolute_error: 1.1568\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 4.4365 - mean_absolute_error: 1.1579\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.5104 - mean_absolute_error: 1.1445\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.3987 - mean_absolute_error: 1.1299\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.4481 - mean_absolute_error: 1.1366\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.2979 - mean_absolute_error: 1.1273\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.2303 - mean_absolute_error: 1.1192\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.2533 - mean_absolute_error: 1.1277\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.2698 - mean_absolute_error: 1.1032\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 19s 2ms/step - loss: 4.2002 - mean_absolute_error: 1.1014\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 24s 2ms/step - loss: 4.0351 - mean_absolute_error: 1.0810\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 22s 2ms/step - loss: 4.1624 - mean_absolute_error: 1.0914\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 19s 2ms/step - loss: 4.1076 - mean_absolute_error: 1.0851\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 20s 2ms/step - loss: 4.1042 - mean_absolute_error: 1.0869\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 4.0141 - mean_absolute_error: 1.0739\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 4.0598 - mean_absolute_error: 1.0736\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 3.8384 - mean_absolute_error: 1.0590\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 3.8337 - mean_absolute_error: 1.0500\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 4.0610 - mean_absolute_error: 1.0679\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 3.7749 - mean_absolute_error: 1.0323\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 3.7998 - mean_absolute_error: 1.0404\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 19s 2ms/step - loss: 4.0325 - mean_absolute_error: 1.0645\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 22s 2ms/step - loss: 3.7321 - mean_absolute_error: 1.0391\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 21s 2ms/step - loss: 3.6286 - mean_absolute_error: 1.0295\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 3.8911 - mean_absolute_error: 1.0358\n",
      "QWK:  0.9674684772448536\n",
      "Fold # 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_128 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_129 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10382/10382 [==============================] - 100s 10ms/step - loss: 54.6246 - mean_absolute_error: 3.6569\n",
      "Epoch 2/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 30.2020 - mean_absolute_error: 2.5990\n",
      "Epoch 3/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 19.5383 - mean_absolute_error: 2.1679\n",
      "Epoch 4/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 13.1562 - mean_absolute_error: 1.8822\n",
      "Epoch 5/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 9.6989 - mean_absolute_error: 1.6775\n",
      "Epoch 6/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 8.1481 - mean_absolute_error: 1.5562\n",
      "Epoch 7/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 7.8671 - mean_absolute_error: 1.5051\n",
      "Epoch 8/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 6.8695 - mean_absolute_error: 1.4446\n",
      "Epoch 9/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 6.7477 - mean_absolute_error: 1.4331\n",
      "Epoch 10/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 6.6157 - mean_absolute_error: 1.4062\n",
      "Epoch 11/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 6.1819 - mean_absolute_error: 1.3612\n",
      "Epoch 12/50\n",
      "10382/10382 [==============================] - 16s 1ms/step - loss: 6.2311 - mean_absolute_error: 1.3416\n",
      "Epoch 13/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 6.0814 - mean_absolute_error: 1.3637\n",
      "Epoch 14/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 5.6021 - mean_absolute_error: 1.2987\n",
      "Epoch 15/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 5.6002 - mean_absolute_error: 1.3002\n",
      "Epoch 16/50\n",
      "10382/10382 [==============================] - 21s 2ms/step - loss: 5.4114 - mean_absolute_error: 1.2780\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10382/10382 [==============================] - 20s 2ms/step - loss: 5.3736 - mean_absolute_error: 1.2753\n",
      "Epoch 18/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 5.6234 - mean_absolute_error: 1.2786\n",
      "Epoch 19/50\n",
      "10382/10382 [==============================] - 22s 2ms/step - loss: 5.1313 - mean_absolute_error: 1.2421\n",
      "Epoch 20/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 5.2966 - mean_absolute_error: 1.2400\n",
      "Epoch 21/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 5.1436 - mean_absolute_error: 1.2356\n",
      "Epoch 22/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.9231 - mean_absolute_error: 1.1952\n",
      "Epoch 23/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.8398 - mean_absolute_error: 1.1986\n",
      "Epoch 24/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.9563 - mean_absolute_error: 1.1833\n",
      "Epoch 25/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.9015 - mean_absolute_error: 1.1850\n",
      "Epoch 26/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.6708 - mean_absolute_error: 1.1713\n",
      "Epoch 27/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.3040 - mean_absolute_error: 1.1516\n",
      "Epoch 28/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.4402 - mean_absolute_error: 1.1483\n",
      "Epoch 29/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.3043 - mean_absolute_error: 1.1338\n",
      "Epoch 30/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.5110 - mean_absolute_error: 1.1493\n",
      "Epoch 31/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 4.2582 - mean_absolute_error: 1.1336\n",
      "Epoch 32/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 4.3905 - mean_absolute_error: 1.1196\n",
      "Epoch 33/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 4.1364 - mean_absolute_error: 1.1039\n",
      "Epoch 34/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.4197 - mean_absolute_error: 1.1111: 2s - loss: 4.3959 - me\n",
      "Epoch 35/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 4.4619 - mean_absolute_error: 1.1131\n",
      "Epoch 36/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 4.2377 - mean_absolute_error: 1.0974\n",
      "Epoch 37/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 4.1352 - mean_absolute_error: 1.0911\n",
      "Epoch 38/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 3.9696 - mean_absolute_error: 1.0764\n",
      "Epoch 39/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 4.0096 - mean_absolute_error: 1.0818\n",
      "Epoch 40/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.8016 - mean_absolute_error: 1.0554\n",
      "Epoch 41/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 3.7932 - mean_absolute_error: 1.0537\n",
      "Epoch 42/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 3.6839 - mean_absolute_error: 1.0481\n",
      "Epoch 43/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 3.7711 - mean_absolute_error: 1.0509\n",
      "Epoch 44/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.8635 - mean_absolute_error: 1.0421\n",
      "Epoch 45/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.8684 - mean_absolute_error: 1.0487\n",
      "Epoch 46/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 3.9747 - mean_absolute_error: 1.0579\n",
      "Epoch 47/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 3.4715 - mean_absolute_error: 1.0198\n",
      "Epoch 48/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 3.5213 - mean_absolute_error: 1.0240: 4s - loss:\n",
      "Epoch 49/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 3.6792 - mean_absolute_error: 1.0361\n",
      "Epoch 50/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 3.6999 - mean_absolute_error: 1.0239\n",
      "QWK:  0.9727294307657198\n",
      "Fold # 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_130 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_131 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10382/10382 [==============================] - 137s 13ms/step - loss: 53.8546 - mean_absolute_error: 3.6036\n",
      "Epoch 2/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 30.0773 - mean_absolute_error: 2.5956\n",
      "Epoch 3/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 19.7927 - mean_absolute_error: 2.1698\n",
      "Epoch 4/50\n",
      "10382/10382 [==============================] - 21s 2ms/step - loss: 12.7325 - mean_absolute_error: 1.8421\n",
      "Epoch 5/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 9.7461 - mean_absolute_error: 1.6688\n",
      "Epoch 6/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 8.6526 - mean_absolute_error: 1.5797\n",
      "Epoch 7/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 7.9669 - mean_absolute_error: 1.5349\n",
      "Epoch 8/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 7.1787 - mean_absolute_error: 1.4597\n",
      "Epoch 9/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 6.9699 - mean_absolute_error: 1.4444\n",
      "Epoch 10/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 6.9473 - mean_absolute_error: 1.4266\n",
      "Epoch 11/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 6.7479 - mean_absolute_error: 1.4104\n",
      "Epoch 12/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 6.4225 - mean_absolute_error: 1.3678\n",
      "Epoch 13/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 6.1489 - mean_absolute_error: 1.3429\n",
      "Epoch 14/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 6.2620 - mean_absolute_error: 1.3319: 3s - loss:\n",
      "Epoch 15/50\n",
      "10382/10382 [==============================] - 25s 2ms/step - loss: 5.9768 - mean_absolute_error: 1.3190\n",
      "Epoch 16/50\n",
      "10382/10382 [==============================] - 26s 2ms/step - loss: 5.6579 - mean_absolute_error: 1.3074\n",
      "Epoch 17/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 5.5504 - mean_absolute_error: 1.2963\n",
      "Epoch 18/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 5.3474 - mean_absolute_error: 1.2525\n",
      "Epoch 19/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 5.2330 - mean_absolute_error: 1.2393\n",
      "Epoch 20/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 5.3394 - mean_absolute_error: 1.2438\n",
      "Epoch 21/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 4.9723 - mean_absolute_error: 1.2146\n",
      "Epoch 22/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 5.2653 - mean_absolute_error: 1.2274\n",
      "Epoch 23/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 5.0408 - mean_absolute_error: 1.2034\n",
      "Epoch 24/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 4.9444 - mean_absolute_error: 1.2068\n",
      "Epoch 25/50\n",
      "10382/10382 [==============================] - 16s 1ms/step - loss: 5.0590 - mean_absolute_error: 1.1929\n",
      "Epoch 26/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 4.7776 - mean_absolute_error: 1.1840\n",
      "Epoch 27/50\n",
      "10382/10382 [==============================] - 16s 1ms/step - loss: 4.6856 - mean_absolute_error: 1.1705\n",
      "Epoch 28/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.4592 - mean_absolute_error: 1.1530\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10382/10382 [==============================] - 16s 1ms/step - loss: 4.7094 - mean_absolute_error: 1.1610\n",
      "Epoch 30/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.3454 - mean_absolute_error: 1.1439\n",
      "Epoch 31/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.5004 - mean_absolute_error: 1.1391\n",
      "Epoch 32/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.6844 - mean_absolute_error: 1.1449\n",
      "Epoch 33/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.3822 - mean_absolute_error: 1.1202\n",
      "Epoch 34/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.3060 - mean_absolute_error: 1.1138\n",
      "Epoch 35/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 4.2185 - mean_absolute_error: 1.1087\n",
      "Epoch 36/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.0416 - mean_absolute_error: 1.0928\n",
      "Epoch 37/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.2422 - mean_absolute_error: 1.0934\n",
      "Epoch 38/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 4.2783 - mean_absolute_error: 1.1093\n",
      "Epoch 39/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 4.0934 - mean_absolute_error: 1.0834\n",
      "Epoch 40/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.1605 - mean_absolute_error: 1.0742\n",
      "Epoch 41/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 4.1602 - mean_absolute_error: 1.0799\n",
      "Epoch 42/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.8342 - mean_absolute_error: 1.0640\n",
      "Epoch 43/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 4.1023 - mean_absolute_error: 1.0825\n",
      "Epoch 44/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.8373 - mean_absolute_error: 1.0624\n",
      "Epoch 45/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 4.0567 - mean_absolute_error: 1.0742\n",
      "Epoch 46/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.0029 - mean_absolute_error: 1.0579\n",
      "Epoch 47/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.8193 - mean_absolute_error: 1.0390\n",
      "Epoch 48/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.7470 - mean_absolute_error: 1.0425\n",
      "Epoch 49/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.5333 - mean_absolute_error: 1.0279\n",
      "Epoch 50/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 3.7496 - mean_absolute_error: 1.0291\n",
      "QWK:  0.9685796236967704\n",
      "Fold # 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_132 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_133 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10382/10382 [==============================] - 111s 11ms/step - loss: 53.1114 - mean_absolute_error: 3.6089\n",
      "Epoch 2/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 29.1334 - mean_absolute_error: 2.5508\n",
      "Epoch 3/50\n",
      "10382/10382 [==============================] - 27s 3ms/step - loss: 18.9683 - mean_absolute_error: 2.1255\n",
      "Epoch 4/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 13.2502 - mean_absolute_error: 1.8526\n",
      "Epoch 5/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 9.7330 - mean_absolute_error: 1.6571\n",
      "Epoch 6/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 8.1520 - mean_absolute_error: 1.5345\n",
      "Epoch 7/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 7.8017 - mean_absolute_error: 1.5054\n",
      "Epoch 8/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 7.0684 - mean_absolute_error: 1.4477\n",
      "Epoch 9/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 6.7053 - mean_absolute_error: 1.4174\n",
      "Epoch 10/50\n",
      "10382/10382 [==============================] - 21s 2ms/step - loss: 6.8337 - mean_absolute_error: 1.4213\n",
      "Epoch 11/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 6.5438 - mean_absolute_error: 1.3846\n",
      "Epoch 12/50\n",
      "10382/10382 [==============================] - 22s 2ms/step - loss: 6.4697 - mean_absolute_error: 1.3830\n",
      "Epoch 13/50\n",
      "10382/10382 [==============================] - 21s 2ms/step - loss: 6.4049 - mean_absolute_error: 1.3819\n",
      "Epoch 14/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 5.7111 - mean_absolute_error: 1.3080\n",
      "Epoch 15/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 5.9024 - mean_absolute_error: 1.3097\n",
      "Epoch 16/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 5.7034 - mean_absolute_error: 1.2971\n",
      "Epoch 17/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 5.8542 - mean_absolute_error: 1.2921\n",
      "Epoch 18/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 5.3186 - mean_absolute_error: 1.2693\n",
      "Epoch 19/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 5.4310 - mean_absolute_error: 1.2526\n",
      "Epoch 20/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 5.0378 - mean_absolute_error: 1.2143\n",
      "Epoch 21/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 5.1621 - mean_absolute_error: 1.2399\n",
      "Epoch 22/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 5.0739 - mean_absolute_error: 1.2252\n",
      "Epoch 23/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.9350 - mean_absolute_error: 1.2053\n",
      "Epoch 24/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.7923 - mean_absolute_error: 1.1967\n",
      "Epoch 25/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.7587 - mean_absolute_error: 1.1881\n",
      "Epoch 26/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.3431 - mean_absolute_error: 1.1565\n",
      "Epoch 27/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.5252 - mean_absolute_error: 1.1628\n",
      "Epoch 28/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.5868 - mean_absolute_error: 1.1518\n",
      "Epoch 29/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.2600 - mean_absolute_error: 1.1313\n",
      "Epoch 30/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.6250 - mean_absolute_error: 1.1404\n",
      "Epoch 31/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.3230 - mean_absolute_error: 1.1203\n",
      "Epoch 32/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.3860 - mean_absolute_error: 1.1321\n",
      "Epoch 33/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.3105 - mean_absolute_error: 1.1127\n",
      "Epoch 34/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.0680 - mean_absolute_error: 1.0909\n",
      "Epoch 35/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.1687 - mean_absolute_error: 1.0993\n",
      "Epoch 36/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 4.0502 - mean_absolute_error: 1.0823\n",
      "Epoch 37/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.3866 - mean_absolute_error: 1.1015\n",
      "Epoch 38/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.1335 - mean_absolute_error: 1.0785\n",
      "Epoch 39/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.9858 - mean_absolute_error: 1.0784\n",
      "Epoch 40/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.0595 - mean_absolute_error: 1.0827\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10382/10382 [==============================] - 15s 1ms/step - loss: 3.8608 - mean_absolute_error: 1.0606\n",
      "Epoch 42/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 3.9544 - mean_absolute_error: 1.0598\n",
      "Epoch 43/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 3.7177 - mean_absolute_error: 1.0363\n",
      "Epoch 44/50\n",
      "10382/10382 [==============================] - 16s 1ms/step - loss: 3.7886 - mean_absolute_error: 1.0473\n",
      "Epoch 45/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 3.7725 - mean_absolute_error: 1.0450\n",
      "Epoch 46/50\n",
      "10382/10382 [==============================] - 16s 1ms/step - loss: 3.7438 - mean_absolute_error: 1.0449\n",
      "Epoch 47/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 3.8603 - mean_absolute_error: 1.0380\n",
      "Epoch 48/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 3.6756 - mean_absolute_error: 1.0277\n",
      "Epoch 49/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 3.6920 - mean_absolute_error: 1.0378\n",
      "Epoch 50/50\n",
      "10382/10382 [==============================] - 16s 1ms/step - loss: 3.5729 - mean_absolute_error: 1.0267\n",
      "QWK:  0.9747481388630764\n",
      "Average Quadratic Weighted Kappa after 5-fold cross validation for min + max word2vec  0.9713\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "dataset = KFold(len(X), n_folds=5, shuffle=True)\n",
    "\n",
    "results_min_max = train_model(X, y, dataset, \"min+max\")\n",
    "print(\"Average Quadratic Weighted Kappa after 5-fold cross validation for min + max word2vec \",np.around(np.array(results_min_max).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:37: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:38: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_134 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_135 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 94s 9ms/step - loss: 81.9736 - mean_absolute_error: 5.1235\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 50.9084 - mean_absolute_error: 3.9927\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 26s 3ms/step - loss: 41.0253 - mean_absolute_error: 3.6480\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 20s 2ms/step - loss: 37.1916 - mean_absolute_error: 3.6233\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 20s 2ms/step - loss: 35.3798 - mean_absolute_error: 3.6477\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 19s 2ms/step - loss: 33.8305 - mean_absolute_error: 3.5999\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 19s 2ms/step - loss: 33.3558 - mean_absolute_error: 3.6060\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 33.0746 - mean_absolute_error: 3.5868\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 32.7629 - mean_absolute_error: 3.5625\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 32.4429 - mean_absolute_error: 3.4997\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 31.9444 - mean_absolute_error: 3.4517\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 31.5249 - mean_absolute_error: 3.4422\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 31.3329 - mean_absolute_error: 3.3681\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 30.5936 - mean_absolute_error: 3.3113\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 30.4906 - mean_absolute_error: 3.2553\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 21s 2ms/step - loss: 30.3630 - mean_absolute_error: 3.2318\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 19s 2ms/step - loss: 29.0776 - mean_absolute_error: 3.1605\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 28.5902 - mean_absolute_error: 3.1016\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 27.6140 - mean_absolute_error: 3.0510\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 26.8817 - mean_absolute_error: 3.0076\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 26.6870 - mean_absolute_error: 2.9878\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 24.6766 - mean_absolute_error: 2.9090\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 24.7427 - mean_absolute_error: 2.8849\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 23.7333 - mean_absolute_error: 2.8058\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 22.7003 - mean_absolute_error: 2.7346\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 22.7965 - mean_absolute_error: 2.7425\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 22.6762 - mean_absolute_error: 2.7498\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 22.3907 - mean_absolute_error: 2.7173\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 22.9469 - mean_absolute_error: 2.7066\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 22.4089 - mean_absolute_error: 2.7098\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 22.6216 - mean_absolute_error: 2.7158\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 21.4556 - mean_absolute_error: 2.6596\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 21.8187 - mean_absolute_error: 2.6646\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 21.3781 - mean_absolute_error: 2.6533\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 21.7703 - mean_absolute_error: 2.6605\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 20.5146 - mean_absolute_error: 2.6047\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 20.8509 - mean_absolute_error: 2.6072\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 20.4640 - mean_absolute_error: 2.5941\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 20.8565 - mean_absolute_error: 2.6013\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 20.1031 - mean_absolute_error: 2.5636\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 20.5930 - mean_absolute_error: 2.5828\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 19.8360 - mean_absolute_error: 2.5551\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 19.9160 - mean_absolute_error: 2.5435\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 19.8822 - mean_absolute_error: 2.5126\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 19.6900 - mean_absolute_error: 2.5069\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 18.4102 - mean_absolute_error: 2.4363\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 18.4341 - mean_absolute_error: 2.4046\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 21s 2ms/step - loss: 18.6557 - mean_absolute_error: 2.3932\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 23s 2ms/step - loss: 18.7355 - mean_absolute_error: 2.3953\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 19s 2ms/step - loss: 18.0207 - mean_absolute_error: 2.3433\n",
      "QWK:  0.9098997098906424\n",
      "Fold # 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_136 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_137 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 199s 19ms/step - loss: 80.2300 - mean_absolute_error: 5.1069\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 49.8479 - mean_absolute_error: 3.9610\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 40.6351 - mean_absolute_error: 3.6653\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 36.7082 - mean_absolute_error: 3.6234\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10381/10381 [==============================] - 15s 1ms/step - loss: 34.6254 - mean_absolute_error: 3.6431\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 33.7463 - mean_absolute_error: 3.6157\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 23s 2ms/step - loss: 33.9325 - mean_absolute_error: 3.6317\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 32.0536 - mean_absolute_error: 3.5334\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 32.3427 - mean_absolute_error: 3.5498\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 19s 2ms/step - loss: 31.3057 - mean_absolute_error: 3.4952\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 30.8124 - mean_absolute_error: 3.4609\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 31.2868 - mean_absolute_error: 3.3974\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 24s 2ms/step - loss: 30.1001 - mean_absolute_error: 3.3540\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 30.7892 - mean_absolute_error: 3.3236\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 30.6821 - mean_absolute_error: 3.2744\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 29.8200 - mean_absolute_error: 3.2416\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 16s 1ms/step - loss: 29.7808 - mean_absolute_error: 3.2054\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 28.9012 - mean_absolute_error: 3.1412\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 27.9909 - mean_absolute_error: 3.0885\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 28.2516 - mean_absolute_error: 3.0933\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 26.1862 - mean_absolute_error: 2.9800\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 25.0364 - mean_absolute_error: 2.9048\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 23.7317 - mean_absolute_error: 2.8472\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 22s 2ms/step - loss: 23.7432 - mean_absolute_error: 2.8201\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 21s 2ms/step - loss: 22.9061 - mean_absolute_error: 2.7883\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 23.1087 - mean_absolute_error: 2.7728\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 22.3100 - mean_absolute_error: 2.7203\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 21.8141 - mean_absolute_error: 2.6967\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 21.4729 - mean_absolute_error: 2.6637\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 21.7154 - mean_absolute_error: 2.6736\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 16s 1ms/step - loss: 21.2916 - mean_absolute_error: 2.6621\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 21.3360 - mean_absolute_error: 2.6445\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 20s 2ms/step - loss: 20.9074 - mean_absolute_error: 2.6220\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 20s 2ms/step - loss: 20.9584 - mean_absolute_error: 2.6379\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 20.5958 - mean_absolute_error: 2.6055\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 20.2272 - mean_absolute_error: 2.6173\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 20.2722 - mean_absolute_error: 2.6022\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 19.8984 - mean_absolute_error: 2.5704\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 19.7278 - mean_absolute_error: 2.5847\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 19.7754 - mean_absolute_error: 2.5617\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 20.3119 - mean_absolute_error: 2.5799\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 20.0177 - mean_absolute_error: 2.5518\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 18.6056 - mean_absolute_error: 2.4987\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 19.9954 - mean_absolute_error: 2.5627\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 19.0304 - mean_absolute_error: 2.4949\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 19.8093 - mean_absolute_error: 2.5220\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 19.1101 - mean_absolute_error: 2.4893\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 18.0609 - mean_absolute_error: 2.4506\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 18.0889 - mean_absolute_error: 2.4249\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 18.5230 - mean_absolute_error: 2.4363\n",
      "QWK:  0.9027899887321981\n",
      "Fold # 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_138 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_139 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10382/10382 [==============================] - 89s 9ms/step - loss: 77.7409 - mean_absolute_error: 5.0247\n",
      "Epoch 2/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 48.0419 - mean_absolute_error: 3.9072\n",
      "Epoch 3/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 39.0363 - mean_absolute_error: 3.6405\n",
      "Epoch 4/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 35.8234 - mean_absolute_error: 3.6182\n",
      "Epoch 5/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 34.6118 - mean_absolute_error: 3.6343\n",
      "Epoch 6/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 33.8521 - mean_absolute_error: 3.6256\n",
      "Epoch 7/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 32.9875 - mean_absolute_error: 3.5829\n",
      "Epoch 8/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 32.6508 - mean_absolute_error: 3.5376\n",
      "Epoch 9/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 31.9742 - mean_absolute_error: 3.5549\n",
      "Epoch 10/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 31.2833 - mean_absolute_error: 3.4940\n",
      "Epoch 11/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 31.3146 - mean_absolute_error: 3.4425\n",
      "Epoch 12/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 31.6159 - mean_absolute_error: 3.4470\n",
      "Epoch 13/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 30.9363 - mean_absolute_error: 3.3870\n",
      "Epoch 14/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 30.1964 - mean_absolute_error: 3.2951\n",
      "Epoch 15/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 30.0101 - mean_absolute_error: 3.2614\n",
      "Epoch 16/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 29.7232 - mean_absolute_error: 3.2171\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10382/10382 [==============================] - 15s 1ms/step - loss: 28.7221 - mean_absolute_error: 3.1832\n",
      "Epoch 18/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 28.3017 - mean_absolute_error: 3.1412\n",
      "Epoch 19/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 28.1019 - mean_absolute_error: 3.1025\n",
      "Epoch 20/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 27.0303 - mean_absolute_error: 3.0524\n",
      "Epoch 21/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 25.7259 - mean_absolute_error: 2.9678\n",
      "Epoch 22/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 25.1608 - mean_absolute_error: 2.9198\n",
      "Epoch 23/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 24.5628 - mean_absolute_error: 2.9190\n",
      "Epoch 24/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 22.8968 - mean_absolute_error: 2.8167\n",
      "Epoch 25/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 22.9910 - mean_absolute_error: 2.7899\n",
      "Epoch 26/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 22.5257 - mean_absolute_error: 2.7674\n",
      "Epoch 27/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 22.8256 - mean_absolute_error: 2.7425\n",
      "Epoch 28/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 22.2734 - mean_absolute_error: 2.7214\n",
      "Epoch 29/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 21.9698 - mean_absolute_error: 2.7037\n",
      "Epoch 30/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 21.3356 - mean_absolute_error: 2.6617\n",
      "Epoch 31/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 21.4670 - mean_absolute_error: 2.6792\n",
      "Epoch 32/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 21.0443 - mean_absolute_error: 2.6576\n",
      "Epoch 33/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 20.8458 - mean_absolute_error: 2.6283\n",
      "Epoch 34/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 21.3678 - mean_absolute_error: 2.6360\n",
      "Epoch 35/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 21.3726 - mean_absolute_error: 2.6495\n",
      "Epoch 36/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 20.5138 - mean_absolute_error: 2.5893\n",
      "Epoch 37/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 20.3405 - mean_absolute_error: 2.6031\n",
      "Epoch 38/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 20.9209 - mean_absolute_error: 2.6320\n",
      "Epoch 39/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 19.7491 - mean_absolute_error: 2.5858\n",
      "Epoch 40/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 20.5685 - mean_absolute_error: 2.5938\n",
      "Epoch 41/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 20.0102 - mean_absolute_error: 2.5539\n",
      "Epoch 42/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 19.5518 - mean_absolute_error: 2.5349\n",
      "Epoch 43/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 19.3867 - mean_absolute_error: 2.5303\n",
      "Epoch 44/50\n",
      "10382/10382 [==============================] - 16s 1ms/step - loss: 19.1834 - mean_absolute_error: 2.5081\n",
      "Epoch 45/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 19.6833 - mean_absolute_error: 2.5173\n",
      "Epoch 46/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 19.1880 - mean_absolute_error: 2.5038\n",
      "Epoch 47/50\n",
      "10382/10382 [==============================] - 22s 2ms/step - loss: 18.2540 - mean_absolute_error: 2.4507\n",
      "Epoch 48/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 18.6043 - mean_absolute_error: 2.4575\n",
      "Epoch 49/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 18.1173 - mean_absolute_error: 2.4053\n",
      "Epoch 50/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 17.4088 - mean_absolute_error: 2.3732\n",
      "QWK:  0.9000602569154881\n",
      "Fold # 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_140 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_141 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10382/10382 [==============================] - 78s 8ms/step - loss: 79.8496 - mean_absolute_error: 5.0460\n",
      "Epoch 2/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 49.7546 - mean_absolute_error: 3.9590\n",
      "Epoch 3/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 40.0584 - mean_absolute_error: 3.6256\n",
      "Epoch 4/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 35.4848 - mean_absolute_error: 3.5906\n",
      "Epoch 5/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 34.7751 - mean_absolute_error: 3.6072\n",
      "Epoch 6/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 34.0128 - mean_absolute_error: 3.5951\n",
      "Epoch 7/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 33.5125 - mean_absolute_error: 3.5761\n",
      "Epoch 8/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 31.7748 - mean_absolute_error: 3.5307\n",
      "Epoch 9/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 32.1318 - mean_absolute_error: 3.5230\n",
      "Epoch 10/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 31.4607 - mean_absolute_error: 3.4525\n",
      "Epoch 11/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 31.8882 - mean_absolute_error: 3.4698\n",
      "Epoch 12/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 31.1393 - mean_absolute_error: 3.3888\n",
      "Epoch 13/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 30.3979 - mean_absolute_error: 3.3344\n",
      "Epoch 14/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 30.4051 - mean_absolute_error: 3.2836\n",
      "Epoch 15/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 30.3228 - mean_absolute_error: 3.2574\n",
      "Epoch 16/50\n",
      "10382/10382 [==============================] - 22s 2ms/step - loss: 29.6156 - mean_absolute_error: 3.2196\n",
      "Epoch 17/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 29.2166 - mean_absolute_error: 3.1421\n",
      "Epoch 18/50\n",
      "10382/10382 [==============================] - 22s 2ms/step - loss: 28.8879 - mean_absolute_error: 3.1369\n",
      "Epoch 19/50\n",
      "10382/10382 [==============================] - 23s 2ms/step - loss: 28.9363 - mean_absolute_error: 3.1282\n",
      "Epoch 20/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 27.5210 - mean_absolute_error: 3.0093\n",
      "Epoch 21/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 27.2415 - mean_absolute_error: 3.0263\n",
      "Epoch 22/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 25.3928 - mean_absolute_error: 2.9459\n",
      "Epoch 23/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 26.0298 - mean_absolute_error: 2.9620\n",
      "Epoch 24/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 23.9206 - mean_absolute_error: 2.8442\n",
      "Epoch 25/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 22.9305 - mean_absolute_error: 2.7940\n",
      "Epoch 26/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 23.1864 - mean_absolute_error: 2.7803\n",
      "Epoch 27/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 23.3888 - mean_absolute_error: 2.7740\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10382/10382 [==============================] - 16s 2ms/step - loss: 21.7237 - mean_absolute_error: 2.6945\n",
      "Epoch 29/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 22.9782 - mean_absolute_error: 2.7364\n",
      "Epoch 30/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 21.8549 - mean_absolute_error: 2.6861\n",
      "Epoch 31/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 21.8700 - mean_absolute_error: 2.6581\n",
      "Epoch 32/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 22.0280 - mean_absolute_error: 2.6866\n",
      "Epoch 33/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 21.0178 - mean_absolute_error: 2.6408\n",
      "Epoch 34/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 20.2378 - mean_absolute_error: 2.6053\n",
      "Epoch 35/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 21.2524 - mean_absolute_error: 2.6175\n",
      "Epoch 36/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 20.9085 - mean_absolute_error: 2.6324\n",
      "Epoch 37/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 20.5098 - mean_absolute_error: 2.6007\n",
      "Epoch 38/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 20.3485 - mean_absolute_error: 2.5810\n",
      "Epoch 39/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 20.3828 - mean_absolute_error: 2.5794\n",
      "Epoch 40/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 19.6269 - mean_absolute_error: 2.5750\n",
      "Epoch 41/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 20.7661 - mean_absolute_error: 2.5916\n",
      "Epoch 42/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 19.9643 - mean_absolute_error: 2.5616\n",
      "Epoch 43/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 19.0209 - mean_absolute_error: 2.5133\n",
      "Epoch 44/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 19.0025 - mean_absolute_error: 2.4874\n",
      "Epoch 45/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 19.1634 - mean_absolute_error: 2.5042\n",
      "Epoch 46/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 18.7938 - mean_absolute_error: 2.4619\n",
      "Epoch 47/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 19.1084 - mean_absolute_error: 2.4695\n",
      "Epoch 48/50\n",
      "10382/10382 [==============================] - 21s 2ms/step - loss: 18.5027 - mean_absolute_error: 2.4156\n",
      "Epoch 49/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 18.6196 - mean_absolute_error: 2.4201\n",
      "Epoch 50/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 17.1091 - mean_absolute_error: 2.3320\n",
      "QWK:  0.9060309973632426\n",
      "Fold # 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_142 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_143 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10382/10382 [==============================] - 261s 25ms/step - loss: 79.3596 - mean_absolute_error: 5.0747\n",
      "Epoch 2/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 49.7248 - mean_absolute_error: 3.9379\n",
      "Epoch 3/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 41.0551 - mean_absolute_error: 3.6572\n",
      "Epoch 4/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 36.9376 - mean_absolute_error: 3.6105\n",
      "Epoch 5/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 35.5268 - mean_absolute_error: 3.6120\n",
      "Epoch 6/50\n",
      "10382/10382 [==============================] - 22s 2ms/step - loss: 33.8491 - mean_absolute_error: 3.6037\n",
      "Epoch 7/50\n",
      "10382/10382 [==============================] - 21s 2ms/step - loss: 33.4407 - mean_absolute_error: 3.5893\n",
      "Epoch 8/50\n",
      "10382/10382 [==============================] - 21s 2ms/step - loss: 33.4067 - mean_absolute_error: 3.5751\n",
      "Epoch 9/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 31.9948 - mean_absolute_error: 3.5280\n",
      "Epoch 10/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 32.0277 - mean_absolute_error: 3.4960\n",
      "Epoch 11/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 32.0160 - mean_absolute_error: 3.4778\n",
      "Epoch 12/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 31.7818 - mean_absolute_error: 3.4443\n",
      "Epoch 13/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 31.5667 - mean_absolute_error: 3.3936\n",
      "Epoch 14/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 30.9341 - mean_absolute_error: 3.3262\n",
      "Epoch 15/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 31.0410 - mean_absolute_error: 3.2893\n",
      "Epoch 16/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 30.4373 - mean_absolute_error: 3.2319\n",
      "Epoch 17/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 30.4533 - mean_absolute_error: 3.2082\n",
      "Epoch 18/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 29.2596 - mean_absolute_error: 3.1567\n",
      "Epoch 19/50\n",
      "10382/10382 [==============================] - 21s 2ms/step - loss: 29.3454 - mean_absolute_error: 3.1634\n",
      "Epoch 20/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 28.7997 - mean_absolute_error: 3.0939\n",
      "Epoch 21/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 27.0575 - mean_absolute_error: 3.0347\n",
      "Epoch 22/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 25.8268 - mean_absolute_error: 2.9511\n",
      "Epoch 23/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 25.5366 - mean_absolute_error: 2.9075\n",
      "Epoch 24/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 23.2586 - mean_absolute_error: 2.8246\n",
      "Epoch 25/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 23.0916 - mean_absolute_error: 2.8031\n",
      "Epoch 26/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 23.1738 - mean_absolute_error: 2.7637\n",
      "Epoch 27/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 23.3545 - mean_absolute_error: 2.7639\n",
      "Epoch 28/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 22.6938 - mean_absolute_error: 2.7465\n",
      "Epoch 29/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 22.8806 - mean_absolute_error: 2.7328\n",
      "Epoch 30/50\n",
      "10382/10382 [==============================] - 23s 2ms/step - loss: 21.7386 - mean_absolute_error: 2.6957\n",
      "Epoch 31/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 22.7336 - mean_absolute_error: 2.7242\n",
      "Epoch 32/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 21.4557 - mean_absolute_error: 2.6708\n",
      "Epoch 33/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 22.1876 - mean_absolute_error: 2.6811\n",
      "Epoch 34/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 21.7651 - mean_absolute_error: 2.6756\n",
      "Epoch 35/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 21.5515 - mean_absolute_error: 2.6657\n",
      "Epoch 36/50\n",
      "10382/10382 [==============================] - 16s 1ms/step - loss: 21.4322 - mean_absolute_error: 2.6540\n",
      "Epoch 37/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 21.0309 - mean_absolute_error: 2.6396\n",
      "Epoch 38/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 21.0993 - mean_absolute_error: 2.6365\n",
      "Epoch 39/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 20.7119 - mean_absolute_error: 2.5903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 21.1063 - mean_absolute_error: 2.6182\n",
      "Epoch 41/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 21.0738 - mean_absolute_error: 2.6389\n",
      "Epoch 42/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 21.5194 - mean_absolute_error: 2.6338\n",
      "Epoch 43/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 20.2441 - mean_absolute_error: 2.5645\n",
      "Epoch 44/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 20.0195 - mean_absolute_error: 2.5484\n",
      "Epoch 45/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 19.9147 - mean_absolute_error: 2.5428\n",
      "Epoch 46/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 19.8401 - mean_absolute_error: 2.5385\n",
      "Epoch 47/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 19.2514 - mean_absolute_error: 2.5002\n",
      "Epoch 48/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 19.5742 - mean_absolute_error: 2.4860\n",
      "Epoch 49/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 18.4530 - mean_absolute_error: 2.4326\n",
      "Epoch 50/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 19.2483 - mean_absolute_error: 2.4405\n",
      "QWK:  0.9175050916768875\n",
      "Average Quadratic Weighted Kappa after 5-fold cross validation for average word2vec  0.9073\n"
     ]
    }
   ],
   "source": [
    "results_average = train_model(X, y, dataset, \"average\")\n",
    "print(\"Average Quadratic Weighted Kappa after 5-fold cross validation for average word2vec \",np.around(np.array(results_average).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:37: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:38: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_144 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_145 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 182s 18ms/step - loss: 53.5768 - mean_absolute_error: 3.6317\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 25s 2ms/step - loss: 29.7418 - mean_absolute_error: 2.5622\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 19s 2ms/step - loss: 19.3748 - mean_absolute_error: 2.1554\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 13.5434 - mean_absolute_error: 1.8867\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 9.6256 - mean_absolute_error: 1.6522\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 19s 2ms/step - loss: 8.3271 - mean_absolute_error: 1.5605\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 7.6573 - mean_absolute_error: 1.5142\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 20s 2ms/step - loss: 7.3588 - mean_absolute_error: 1.4782\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 21s 2ms/step - loss: 6.7899 - mean_absolute_error: 1.4125\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 6.7919 - mean_absolute_error: 1.4056\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 6.4500 - mean_absolute_error: 1.3981\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 6.2597 - mean_absolute_error: 1.3725\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 5.9955 - mean_absolute_error: 1.3366\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 5.8685 - mean_absolute_error: 1.3409\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 5.8867 - mean_absolute_error: 1.3204\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 5.6593 - mean_absolute_error: 1.2932\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 5.6997 - mean_absolute_error: 1.2835\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 5.5151 - mean_absolute_error: 1.2704\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 5.4062 - mean_absolute_error: 1.2531\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 4.8251 - mean_absolute_error: 1.2165\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 24s 2ms/step - loss: 4.9857 - mean_absolute_error: 1.2178\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 23s 2ms/step - loss: 5.0963 - mean_absolute_error: 1.2153\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 5.3815 - mean_absolute_error: 1.2248\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 4.8489 - mean_absolute_error: 1.1929\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 5.0887 - mean_absolute_error: 1.1956\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 4.5973 - mean_absolute_error: 1.1697\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 4.6270 - mean_absolute_error: 1.1636\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 4.5762 - mean_absolute_error: 1.1596\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.6627 - mean_absolute_error: 1.1645\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 4.4443 - mean_absolute_error: 1.1281\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 4.4587 - mean_absolute_error: 1.1337\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 4.3696 - mean_absolute_error: 1.1180\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 4.4086 - mean_absolute_error: 1.1327\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 3.9867 - mean_absolute_error: 1.0901\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 4.2672 - mean_absolute_error: 1.1107\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.0691 - mean_absolute_error: 1.0782\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.1530 - mean_absolute_error: 1.0936\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.1581 - mean_absolute_error: 1.0908\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 4.0914 - mean_absolute_error: 1.0779\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 4.0344 - mean_absolute_error: 1.0780\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.0502 - mean_absolute_error: 1.0723\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 3.9812 - mean_absolute_error: 1.0615\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 3.9057 - mean_absolute_error: 1.0626\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.0099 - mean_absolute_error: 1.0538\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 3.9454 - mean_absolute_error: 1.0531\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 3.8290 - mean_absolute_error: 1.0424\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 3.8969 - mean_absolute_error: 1.0517\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 3.8219 - mean_absolute_error: 1.0392\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 3.7887 - mean_absolute_error: 1.0317\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 3.6952 - mean_absolute_error: 1.0320\n",
      "QWK:  0.9724160953540747\n",
      "Fold # 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_146 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_147 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 175s 17ms/step - loss: 53.5056 - mean_absolute_error: 3.6353\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 25s 2ms/step - loss: 29.6464 - mean_absolute_error: 2.5586\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 18.9711 - mean_absolute_error: 2.1143\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 12.6603 - mean_absolute_error: 1.8250\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 9.3718 - mean_absolute_error: 1.6361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 7.7867 - mean_absolute_error: 1.5247\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 7.2696 - mean_absolute_error: 1.4773\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 7.3173 - mean_absolute_error: 1.4770\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 6.6282 - mean_absolute_error: 1.4032\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 13s 1ms/step - loss: 6.6489 - mean_absolute_error: 1.3991\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 5.9480 - mean_absolute_error: 1.3525\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 6.1681 - mean_absolute_error: 1.3597\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 5.8158 - mean_absolute_error: 1.3262\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 5.8290 - mean_absolute_error: 1.3187\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 5.6311 - mean_absolute_error: 1.2911\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 5.6566 - mean_absolute_error: 1.2888\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 5.2062 - mean_absolute_error: 1.2617\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 5.3077 - mean_absolute_error: 1.2570\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 5.0758 - mean_absolute_error: 1.2381\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 4.7211 - mean_absolute_error: 1.2032\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 4.8903 - mean_absolute_error: 1.2003\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 4.8691 - mean_absolute_error: 1.1960\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 4.8075 - mean_absolute_error: 1.1848\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 4.6370 - mean_absolute_error: 1.1736\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 20s 2ms/step - loss: 4.6350 - mean_absolute_error: 1.1622\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 4.7754 - mean_absolute_error: 1.1799\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 20s 2ms/step - loss: 4.3107 - mean_absolute_error: 1.1426\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 4.5601 - mean_absolute_error: 1.1589\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 14s 1ms/step - loss: 4.1672 - mean_absolute_error: 1.1208\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 15s 1ms/step - loss: 4.3501 - mean_absolute_error: 1.1253\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 4.0023 - mean_absolute_error: 1.1085\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 19s 2ms/step - loss: 4.1358 - mean_absolute_error: 1.1181\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 21s 2ms/step - loss: 4.3485 - mean_absolute_error: 1.1164\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 19s 2ms/step - loss: 4.3096 - mean_absolute_error: 1.1059\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 3.9325 - mean_absolute_error: 1.0800\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 4.0177 - mean_absolute_error: 1.0830\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 4.2722 - mean_absolute_error: 1.1127\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 3.8403 - mean_absolute_error: 1.0736\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 21s 2ms/step - loss: 3.9911 - mean_absolute_error: 1.0686\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 3.8905 - mean_absolute_error: 1.0653\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 3.9474 - mean_absolute_error: 1.0693\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 3.8001 - mean_absolute_error: 1.0548\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 3.7849 - mean_absolute_error: 1.0523\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 3.8332 - mean_absolute_error: 1.0565\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 3.3639 - mean_absolute_error: 1.0006\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 3.8686 - mean_absolute_error: 1.0471\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 18s 2ms/step - loss: 3.3651 - mean_absolute_error: 1.0123\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 19s 2ms/step - loss: 3.9715 - mean_absolute_error: 1.0454: 1s - loss: 3.9627 - mean_absolute_e\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 16s 2ms/step - loss: 3.5931 - mean_absolute_error: 1.0277\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 17s 2ms/step - loss: 3.2639 - mean_absolute_error: 0.9908\n",
      "QWK:  0.9672305787073104\n",
      "Fold # 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_148 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_149 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10382/10382 [==============================] - 141s 14ms/step - loss: 52.4421 - mean_absolute_error: 3.5997\n",
      "Epoch 2/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 28.4988 - mean_absolute_error: 2.5478\n",
      "Epoch 3/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 18.5808 - mean_absolute_error: 2.1410\n",
      "Epoch 4/50\n",
      "10382/10382 [==============================] - 25s 2ms/step - loss: 12.7613 - mean_absolute_error: 1.8624\n",
      "Epoch 5/50\n",
      "10382/10382 [==============================] - 23s 2ms/step - loss: 9.4084 - mean_absolute_error: 1.6532\n",
      "Epoch 6/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 8.0150 - mean_absolute_error: 1.5462\n",
      "Epoch 7/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 7.7318 - mean_absolute_error: 1.5300\n",
      "Epoch 8/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 7.2428 - mean_absolute_error: 1.4587\n",
      "Epoch 9/50\n",
      "10382/10382 [==============================] - 23s 2ms/step - loss: 6.6222 - mean_absolute_error: 1.4151\n",
      "Epoch 10/50\n",
      "10382/10382 [==============================] - 21s 2ms/step - loss: 6.6790 - mean_absolute_error: 1.4081\n",
      "Epoch 11/50\n",
      "10382/10382 [==============================] - 22s 2ms/step - loss: 6.3781 - mean_absolute_error: 1.3972\n",
      "Epoch 12/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 6.1867 - mean_absolute_error: 1.3672\n",
      "Epoch 13/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 6.2306 - mean_absolute_error: 1.3615\n",
      "Epoch 14/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 5.6185 - mean_absolute_error: 1.3077\n",
      "Epoch 15/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 5.6823 - mean_absolute_error: 1.3046\n",
      "Epoch 16/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 5.5840 - mean_absolute_error: 1.2790\n",
      "Epoch 17/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 5.4942 - mean_absolute_error: 1.2805\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10382/10382 [==============================] - 16s 2ms/step - loss: 5.2942 - mean_absolute_error: 1.2620\n",
      "Epoch 19/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 5.1051 - mean_absolute_error: 1.2279\n",
      "Epoch 20/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 5.2666 - mean_absolute_error: 1.2325\n",
      "Epoch 21/50\n",
      "10382/10382 [==============================] - 25s 2ms/step - loss: 5.2355 - mean_absolute_error: 1.2363\n",
      "Epoch 22/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 5.0512 - mean_absolute_error: 1.2213\n",
      "Epoch 23/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 4.6170 - mean_absolute_error: 1.1757\n",
      "Epoch 24/50\n",
      "10382/10382 [==============================] - 21s 2ms/step - loss: 4.6941 - mean_absolute_error: 1.1853\n",
      "Epoch 25/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 4.8816 - mean_absolute_error: 1.1913\n",
      "Epoch 26/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.5199 - mean_absolute_error: 1.1595\n",
      "Epoch 27/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 4.7594 - mean_absolute_error: 1.1737\n",
      "Epoch 28/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 4.5960 - mean_absolute_error: 1.1604\n",
      "Epoch 29/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 4.7369 - mean_absolute_error: 1.1705\n",
      "Epoch 30/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 4.1455 - mean_absolute_error: 1.1139\n",
      "Epoch 31/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.4037 - mean_absolute_error: 1.1332\n",
      "Epoch 32/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.2836 - mean_absolute_error: 1.1286\n",
      "Epoch 33/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 4.2508 - mean_absolute_error: 1.1055\n",
      "Epoch 34/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.1126 - mean_absolute_error: 1.0962\n",
      "Epoch 35/50\n",
      "10382/10382 [==============================] - 27s 3ms/step - loss: 4.0995 - mean_absolute_error: 1.0904\n",
      "Epoch 36/50\n",
      "10382/10382 [==============================] - 24s 2ms/step - loss: 4.1787 - mean_absolute_error: 1.1013\n",
      "Epoch 37/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 4.2230 - mean_absolute_error: 1.0928\n",
      "Epoch 38/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 3.9349 - mean_absolute_error: 1.0768\n",
      "Epoch 39/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.0728 - mean_absolute_error: 1.0883\n",
      "Epoch 40/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 4.3135 - mean_absolute_error: 1.0936\n",
      "Epoch 41/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 3.8987 - mean_absolute_error: 1.0686\n",
      "Epoch 42/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 3.8729 - mean_absolute_error: 1.0634\n",
      "Epoch 43/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 3.4615 - mean_absolute_error: 1.0206\n",
      "Epoch 44/50\n",
      "10382/10382 [==============================] - 23s 2ms/step - loss: 3.7165 - mean_absolute_error: 1.0337\n",
      "Epoch 45/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 3.9874 - mean_absolute_error: 1.0564\n",
      "Epoch 46/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 3.7347 - mean_absolute_error: 1.0496\n",
      "Epoch 47/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 3.7975 - mean_absolute_error: 1.0468\n",
      "Epoch 48/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 3.6224 - mean_absolute_error: 1.0365\n",
      "Epoch 49/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.6671 - mean_absolute_error: 1.0230\n",
      "Epoch 50/50\n",
      "10382/10382 [==============================] - 22s 2ms/step - loss: 3.6211 - mean_absolute_error: 1.0353\n",
      "QWK:  0.9717122099661926\n",
      "Fold # 4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_150 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_151 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10382/10382 [==============================] - 145s 14ms/step - loss: 53.1420 - mean_absolute_error: 3.5958\n",
      "Epoch 2/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 30.1115 - mean_absolute_error: 2.5869\n",
      "Epoch 3/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 19.6913 - mean_absolute_error: 2.1659\n",
      "Epoch 4/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 13.1694 - mean_absolute_error: 1.8627\n",
      "Epoch 5/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 9.7448 - mean_absolute_error: 1.6632\n",
      "Epoch 6/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 8.4153 - mean_absolute_error: 1.5649\n",
      "Epoch 7/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 7.5012 - mean_absolute_error: 1.4967\n",
      "Epoch 8/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 7.1710 - mean_absolute_error: 1.4564\n",
      "Epoch 9/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 6.8821 - mean_absolute_error: 1.4254\n",
      "Epoch 10/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 6.4564 - mean_absolute_error: 1.3888\n",
      "Epoch 11/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 6.5046 - mean_absolute_error: 1.3829\n",
      "Epoch 12/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 6.5147 - mean_absolute_error: 1.3621\n",
      "Epoch 13/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 5.9192 - mean_absolute_error: 1.3270\n",
      "Epoch 14/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 5.9410 - mean_absolute_error: 1.3303\n",
      "Epoch 15/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 5.5735 - mean_absolute_error: 1.2928\n",
      "Epoch 16/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 5.3217 - mean_absolute_error: 1.2622\n",
      "Epoch 17/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 5.8498 - mean_absolute_error: 1.2907\n",
      "Epoch 18/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 5.2576 - mean_absolute_error: 1.2438\n",
      "Epoch 19/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 5.3610 - mean_absolute_error: 1.2527\n",
      "Epoch 20/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 4.9903 - mean_absolute_error: 1.2233\n",
      "Epoch 21/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.9698 - mean_absolute_error: 1.2175\n",
      "Epoch 22/50\n",
      "10382/10382 [==============================] - 16s 1ms/step - loss: 5.1147 - mean_absolute_error: 1.2161\n",
      "Epoch 23/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 4.8059 - mean_absolute_error: 1.1775\n",
      "Epoch 24/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 4.5756 - mean_absolute_error: 1.1644\n",
      "Epoch 25/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.4066 - mean_absolute_error: 1.1471\n",
      "Epoch 26/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 5.2177 - mean_absolute_error: 1.2160\n",
      "Epoch 27/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.7590 - mean_absolute_error: 1.1564\n",
      "Epoch 28/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.5092 - mean_absolute_error: 1.1412\n",
      "Epoch 29/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.3756 - mean_absolute_error: 1.1351\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.3289 - mean_absolute_error: 1.1237\n",
      "Epoch 31/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 4.6179 - mean_absolute_error: 1.1343\n",
      "Epoch 32/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.2904 - mean_absolute_error: 1.1208\n",
      "Epoch 33/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.0510 - mean_absolute_error: 1.0959\n",
      "Epoch 34/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 3.9973 - mean_absolute_error: 1.0881\n",
      "Epoch 35/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.0035 - mean_absolute_error: 1.0914\n",
      "Epoch 36/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 4.1387 - mean_absolute_error: 1.0897\n",
      "Epoch 37/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.0587 - mean_absolute_error: 1.0826\n",
      "Epoch 38/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 4.1203 - mean_absolute_error: 1.0849\n",
      "Epoch 39/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 3.8819 - mean_absolute_error: 1.0638: 1s - loss: 3.8321 - mean_absolut\n",
      "Epoch 40/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 3.8486 - mean_absolute_error: 1.0587\n",
      "Epoch 41/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 3.9854 - mean_absolute_error: 1.0668\n",
      "Epoch 42/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.7944 - mean_absolute_error: 1.0531\n",
      "Epoch 43/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.8948 - mean_absolute_error: 1.0476\n",
      "Epoch 44/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 3.9515 - mean_absolute_error: 1.0681\n",
      "Epoch 45/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.7594 - mean_absolute_error: 1.0374\n",
      "Epoch 46/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 3.7306 - mean_absolute_error: 1.0404\n",
      "Epoch 47/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 3.8949 - mean_absolute_error: 1.0419\n",
      "Epoch 48/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.8232 - mean_absolute_error: 1.0488\n",
      "Epoch 49/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.8020 - mean_absolute_error: 1.0404\n",
      "Epoch 50/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.7697 - mean_absolute_error: 1.0278\n",
      "QWK:  0.968605139334301\n",
      "Fold # 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_152 (LSTM)              (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_153 (LSTM)              (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10382/10382 [==============================] - 135s 13ms/step - loss: 52.4182 - mean_absolute_error: 3.6050\n",
      "Epoch 2/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 29.0801 - mean_absolute_error: 2.5659\n",
      "Epoch 3/50\n",
      "10382/10382 [==============================] - 16s 1ms/step - loss: 19.4530 - mean_absolute_error: 2.1527\n",
      "Epoch 4/50\n",
      "10382/10382 [==============================] - 16s 1ms/step - loss: 12.8548 - mean_absolute_error: 1.8559\n",
      "Epoch 5/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 9.9784 - mean_absolute_error: 1.6781\n",
      "Epoch 6/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 8.2028 - mean_absolute_error: 1.5688\n",
      "Epoch 7/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 7.7344 - mean_absolute_error: 1.5200\n",
      "Epoch 8/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 7.0058 - mean_absolute_error: 1.4436\n",
      "Epoch 9/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 6.9016 - mean_absolute_error: 1.4374\n",
      "Epoch 10/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 6.8543 - mean_absolute_error: 1.4136\n",
      "Epoch 11/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 6.3578 - mean_absolute_error: 1.3810\n",
      "Epoch 12/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 6.1916 - mean_absolute_error: 1.3626\n",
      "Epoch 13/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 6.0743 - mean_absolute_error: 1.3387\n",
      "Epoch 14/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 5.6986 - mean_absolute_error: 1.3052\n",
      "Epoch 15/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 5.9215 - mean_absolute_error: 1.3214\n",
      "Epoch 16/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 5.6794 - mean_absolute_error: 1.3046\n",
      "Epoch 17/50\n",
      "10382/10382 [==============================] - 16s 2ms/step - loss: 5.5847 - mean_absolute_error: 1.2902\n",
      "Epoch 18/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 5.3734 - mean_absolute_error: 1.2520\n",
      "Epoch 19/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 5.2825 - mean_absolute_error: 1.2472\n",
      "Epoch 20/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 5.0931 - mean_absolute_error: 1.2347\n",
      "Epoch 21/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 5.0517 - mean_absolute_error: 1.2245\n",
      "Epoch 22/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.9712 - mean_absolute_error: 1.2139\n",
      "Epoch 23/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 5.1611 - mean_absolute_error: 1.2263\n",
      "Epoch 24/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.8677 - mean_absolute_error: 1.1848\n",
      "Epoch 25/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.4523 - mean_absolute_error: 1.1634\n",
      "Epoch 26/50\n",
      "10382/10382 [==============================] - 16s 1ms/step - loss: 4.7116 - mean_absolute_error: 1.1697\n",
      "Epoch 27/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.6642 - mean_absolute_error: 1.1654\n",
      "Epoch 28/50\n",
      "10382/10382 [==============================] - 19s 2ms/step - loss: 4.6359 - mean_absolute_error: 1.1571\n",
      "Epoch 29/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 4.7064 - mean_absolute_error: 1.1522\n",
      "Epoch 30/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.3612 - mean_absolute_error: 1.1213\n",
      "Epoch 31/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.4147 - mean_absolute_error: 1.1302\n",
      "Epoch 32/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 4.3288 - mean_absolute_error: 1.1261\n",
      "Epoch 33/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 4.2541 - mean_absolute_error: 1.1065\n",
      "Epoch 34/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.4115 - mean_absolute_error: 1.1174\n",
      "Epoch 35/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.1837 - mean_absolute_error: 1.0945\n",
      "Epoch 36/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.9379 - mean_absolute_error: 1.0841\n",
      "Epoch 37/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 4.1797 - mean_absolute_error: 1.0956\n",
      "Epoch 38/50\n",
      "10382/10382 [==============================] - 16s 1ms/step - loss: 4.1481 - mean_absolute_error: 1.0861\n",
      "Epoch 39/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.1319 - mean_absolute_error: 1.0827\n",
      "Epoch 40/50\n",
      "10382/10382 [==============================] - 14s 1ms/step - loss: 3.9491 - mean_absolute_error: 1.0730\n",
      "Epoch 41/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 3.6994 - mean_absolute_error: 1.0375\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10382/10382 [==============================] - 15s 1ms/step - loss: 3.8653 - mean_absolute_error: 1.0479\n",
      "Epoch 43/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.8314 - mean_absolute_error: 1.0459\n",
      "Epoch 44/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 4.0223 - mean_absolute_error: 1.0663\n",
      "Epoch 45/50\n",
      "10382/10382 [==============================] - 18s 2ms/step - loss: 3.9706 - mean_absolute_error: 1.0520\n",
      "Epoch 46/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.7043 - mean_absolute_error: 1.0467\n",
      "Epoch 47/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.7092 - mean_absolute_error: 1.0337\n",
      "Epoch 48/50\n",
      "10382/10382 [==============================] - 17s 2ms/step - loss: 3.7539 - mean_absolute_error: 1.0317\n",
      "Epoch 49/50\n",
      "10382/10382 [==============================] - 15s 1ms/step - loss: 3.7099 - mean_absolute_error: 1.0442\n",
      "Epoch 50/50\n",
      "10382/10382 [==============================] - 20s 2ms/step - loss: 3.5324 - mean_absolute_error: 1.0108\n",
      "QWK:  0.9731241327645618\n",
      "Average Quadratic Weighted Kappa after 5-fold cross validation for sum word2vec  0.9706\n"
     ]
    }
   ],
   "source": [
    "results_sum = train_model(X, y, dataset)\n",
    "print(\"Average Quadratic Weighted Kappa after 5-fold cross validation for sum word2vec \",np.around(np.array(results_sum).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Quadratic Weighted Kappa after 5-fold cross validation for min + max word2vec  0.9713\n",
      "Average Quadratic Weighted Kappa after 5-fold cross validation for average word2vec  0.9073\n",
      "Average Quadratic Weighted Kappa after 5-fold cross validation for sum word2vec  0.9706\n"
     ]
    }
   ],
   "source": [
    "# Print final results\n",
    "print(\"Average Quadratic Weighted Kappa after 5-fold cross validation for min + max word2vec \",np.around(np.array(results_min_max).mean(),decimals=4))\n",
    "print(\"Average Quadratic Weighted Kappa after 5-fold cross validation for average word2vec \",np.around(np.array(results_average).mean(),decimals=4))\n",
    "print(\"Average Quadratic Weighted Kappa after 5-fold cross validation for sum word2vec \",np.around(np.array(results_sum).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Setup\n",
    "\n",
    "## K-fold cross validation\n",
    "Each model was trained on 80% of the testing set, and testing was performed on the final 20%. For each model, 5-fold cross validation was used for training and testing to avoid over-fitting. 5-fold cross-validation involves randomly partitioning the set into 5 equal sized subsets, of which a single subset is used as validation data and the remaining 4 subsets are used for testing. Then, the cross-validation process is repeated 5 times so that each of the subsets are used as the validation data once and only once. This helps reduce over-fitting by using each sample for both training and validation. Furthermore, it was used for most of the other models in this area, so it helped to even the playing field for a more fair comparison between different models.\n",
    "\n",
    "## Metrics\n",
    "The evaluation metric was Quadratic Weighted Kappa, QWK, as per the ASAP competition. It takes into account as the baseline the possibility of agreement occuring by chance, and it typically varies from 0 (random agreement) to 1 (complete agreement). It is also possible to get a negative score if there is less agreeement than expected by chance. It is calculated between the model's predictions for the scores and the human grading scores for each essay. The QWK for each model is reported as the average from the five fold cross validation.\n",
    "\n",
    "\\begin{equation}\n",
    "k = 1 - \\frac{{\\sum_{i, j}}{W_{i, j}}{O_{i, j}}}{{\\sum_{i, j}}{W_{i, j}}{E_{i, j}}}\n",
    "\\end{equation}\n",
    "\n",
    "# Results\n",
    "It is worth noting that the Kaggle competition had a test set for which the ground truth was not publicly released. Because of this, we could only test by using 20% of the training set as a testing set. This means it is not a fair comparison to directly compare our results with the Kaggle competition. Results for the models are as follows, all of which are based on a 5-fold cross validation using 80% of the training data for training and 20% for testing. \n",
    "\n",
    "\n",
    "| Model  |  QWK Score | \n",
    "|---|---|\n",
    "|Kaggle competition best score|0.801|\n",
    "|Human grading|0.860 | \n",
    "|Logistic regression|0.847|\n",
    "|State of the art (LSTM) |0.961 |\n",
    "|Average Word2Vec and LSTM| 0.907|\n",
    "|Min+max Word2Vec and LSTM|0.971|\n",
    "|Sum Word2Vec and LSTM|0.971|\n",
    "\n",
    "\n",
    "# Analysis of Results\n",
    "The best score during the Kaggle competition was a QWK of 0.801. The Kaggle competition had a larger, hidden test set for which no gold standard is available, so not being able to test on that set is a contributer to our score being so high. The state of the art performance on this dataset was 0.96 in research and 0.961 from open-source work. We built upon their LSTM architecture and used the hyperparameters from the state of the art, and performed better both by taking the min plus the max of the vector representations of the essays (0.971) and got the highest score by using the sum of the vector representations of the sentences (0.971). \n",
    "\n",
    "# Future Work\n",
    "Given the limitations of the dataset, it would be nice to have more robust data to test the models on. For example, four of the datasets were graded for content and not writing ability. Writing quality is a very important measure to take essay score into account, so it would be more beneficial to train the models on essay sets that were graded based on writing ability. We also only used the first human grader's score as the score, since a lot of essays did not include a second human grader score. Testing the models on the average of both scores would allow for a better comparison between human grading and the machine grading. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
